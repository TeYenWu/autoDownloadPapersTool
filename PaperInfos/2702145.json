{"paperId":2702145,"citation":[],"abstract":"Crowdsourcing systems lack effective measures of the effort required to complete each task. Without knowing how much time workers need to execute a task well, requesters struggle to accurately structure and price their work. Objective measures of effort could better help workers identify tasks that are worth their time. We propose a data-driven effort metric, ETA (error-time area), that can be used to determine a task's fair price. It empirically models the relationship between time and error rate by manipulating the time that workers have to complete a task. ETA reports the area under the error-time curve as a continuous metric of worker effort. The curve's 10th percentile is also interpretable as the minimum time most workers require to complete the task without error, which can be used to price the task. We validate the ETA metric on ten common crowdsourcing tasks, including tagging, transcription, and search, and find that ETA closely tracks how workers would rank these tasks by effort. We also demonstrate how ETA allows requesters to rapidly iterate on task designs and measure whether the changes improve worker efficiency. Our findings can facilitate the process of designing, pricing, and allocating crowdsourcing tasks.","reference":[{"content":"Japanese-English Bilingual Corpus of Wikipedia's Kyoto Articles. http://goo.gl/4SfnUi.","paperID":"None"},{"content":"Alwin, D. F., and Krosnick, J. A. The measurement of values in surveys: A comparison of ratings and rankings. Public Opinion Quarterly (1985).","paperID":"None"},{"content":"Anderson, J. R., Matessa, M., and Lebiere, C. ACT-R: A theory of higher level cognition and its relation to visual attention. HCI (1997).","paperID":"None"},{"content":"Bernstein, M. S., et al. Soylent: a word processor with a crowd inside. In UIST (2010).","paperID":"None"},{"content":"Czerwinski, M., Horvitz, E., and Cutrell, E. Subjective duration assessment: An implicit probe for software usability. In IHM-HCI (2001).","paperID":"None"},{"content":"Dai, P., et al. Decision-theoretic control of crowd-sourced workflows. In AAAI (2010).","paperID":"None"},{"content":"DeLeeuw, K. E., and Mayer, R. E. A comparison of three measures of cognitive load: Evidence for separable measures of intrinsic, extraneous, and germane load. J. Educ. Psychol. (2008).","paperID":"None"},{"content":"Faradani, S., Hartmann, B., and Ipeirotis, P. G. What's the right price? pricing tasks for finishing on time. Human Computation (2011).","paperID":"None"},{"content":"Gevins, A., and Smith, M. E. Neurophysiological measures of cognitive workload during human-computer interaction. Theor. Issues. Ergon. (2003).","paperID":"None"},{"content":"Hart, S. G., and Staveland, L. E. Development of NASA-TLX (Task Load Index): Results of empirical and theoretical research. Adv. Psychol. (1988).","paperID":"None"},{"content":"Herlocker, J., Konstan, J. A., and Riedl, J. An empirical analysis of design choices in neighborhood-based collaborative ?ltering algorithms. Inform. Retrieval (2002).","paperID":"None"},{"content":"Hinds, P. J. The curse of expertise: The effects of expertise and debiasing methods on prediction of novice performance. J. Exp. Psychol.-Appl. (1999).","paperID":"None"},{"content":"Impara, J. C., and Plake, B. S. Teachers' ability to estimate item difficulty: A test of the assumptions in the angoff standard setting method. J. Educ. Meas. (1998).","paperID":"None"},{"content":"Ipeirotis, P. G. Mechanical Turk, low wages, and the market for lemons. http://goo.gl/uoXA6Y.","paperID":"None"},{"content":"Ipeirotis, P. G. Analyzing the Amazon Mechanical Turk marketplace. XRDS (2010).","paperID":"None"},{"content":"Ipeirotis, P. G., Provost, F., and Wang, J. Quality management on Amazon Mechanical Turk. In KDD (2010).","paperID":"None"},{"content":"Iyengar, S. S., and Lepper, M. R. When choice is demotivating: Can one desire too much of a good thing? J. Pers. Soc. Psychol. (2000).","paperID":"None"},{"content":"Joglekar, M., Garcia-Molina, H., and Parameswaran, A. Evaluating the crowd with confidence. In KDD (2013).","paperID":"None"},{"content":"Kahneman, D. Thinking, fast and slow. 2011.","paperID":"None"},{"content":"Kittur, A., et al. The future of crowd work. In CSCW (2013).","paperID":"None"},{"content":"Kruger, J., and Dunning, D. Unskilled and unaware of it: how difficulties in recognizing one's own incompetence lead to inflated self-assessments. J. Pers. Soc. Psychol. (1999).","paperID":"None"},{"content":"Lohman, D. F. The effect of speed-accuracy tradeoff on sex differences in mental rotation. Percept. Psychophys. (1986).","paperID":"None"},{"content":"Mao, A., et al. Volunteering versus work for pay: Incentives and tradeoffs in crowdsourcing. In HCOMP (2013).","paperID":"None"},{"content":"Mason, W., and Watts, D. J. Financial incentives and the performance of crowds. SigKDD Explorations (2010).","paperID":"None"},{"content":"McCracken, J., and Aldrich, T. Analyses of selected LHX mission functions: Implications for operator workload and system automation goals. Tech. rep., 1984.","paperID":"None"},{"content":"Pew, R. W. The speed-accuracy operating characteristic. Acta Psychol. (1969).","paperID":"None"},{"content":"Robinson, P. Task complexity, task difficulty, and task production: Exploring interactions in a componential framework. Appl. Linguist. (2001).","paperID":"None"},{"content":"Rzeszotarski, J. M., et al. Inserting micro-breaks into crowdsourcing workflows. In HCOMP (2013).","paperID":"None"},{"content":"Rzeszotarski, J. M., and Kittur, A. Instrumenting the crowd: using implicit behavioral measures to predict task performance. In UIST (2011).","paperID":"None"},{"content":"Toomim, M., Kriplean, T., PÃ¶rtner, C., and Landay, J. Utility of human-computer interactions: Toward a science of preference measurement. In CHI (2011).","paperID":"None"},{"content":"Welinder, P., et al. The multidimensional wisdom of crowds. In NIPS (2010).","paperID":"None"},{"content":"Wickelgren, W. A. Speed-accuracy tradeoff and information processing dynamics. Acta Psychol. (1977).","paperID":"None"},{"content":"Wickens, C. D. Processing resources in attention, dual task performance, and workload assessment. 1981.","paperID":"None"},{"content":"Yao, B., et al. Human action recognition by learning bases of action attributes and parts. In ICCV (2011).","paperID":"None"}],"title":"Measuring Crowdsourcing Effort with Error-Time Curves","filename":"CHI15/p1365","authors":["Justin Cheng","Jaime Teevan","Michael S. Bernstein"],"conference":"CHI '15"}