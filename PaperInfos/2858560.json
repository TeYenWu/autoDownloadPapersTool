{"paperId":2858560,"citation":[],"abstract":"Crowd work provides solutions to complex problems effectively, efficiently, and at low cost. Previous research showed that feedback, particularly correctness feedback can help crowd workers improve their performance; yet such feedback, particularly when generated by experts, is costly and difficult to scale. In our research we investigate approaches to facilitating continuous observational learning in crowdsourcing communities. In a study conducted with workers on Amazon Mechanical Turk, we asked workers to complete a set of tasks identifying nutritional composition of different meals. We examined workers' accuracy gains after being exposed to expert-generated feedback and to two types of peer-generated feedback: direct accuracy assessment with explanations of errors, and a comparison with solutions generated by other workers. The study further confirmed that expert-generated feedback is a powerful mechanism for facilitating learning and leads to significant gains in accuracy. However, the study also showed that comparing one's own solutions with a variety of solutions suggested by others and their comparative frequencies leads to significant gains in accuracy. This solution is particularly attractive because of its low cost, minimal impact on time and cost of job completion, and high potential for adoption by a variety of crowdsourcing platforms.","reference":[{"content":"Luis von Ahn. 2009. Human computation. Proceedings of the ACM International Conference on Image and Video Retrieval, ACM, 1:1--1:1. http://doi.org/10.1145/1646396.1646398","paperID":"None"},{"content":"Luis von Ahn. 2013. Duolingo: learn a language for free while helping to translate the web. Proceedings of the 2013 international conference on Intelligent user interfaces, ACM, 1--2. http://doi.org/10.1145/2449396.2449398","paperID":"None"},{"content":"John Annett. 1969. Feedback and Human Behaviour: The Effects of Knowledge of Results, Incentives and Reinforcement on Learning and Performance. Penguin Books Ltd, Harmondsworth.","paperID":"None"},{"content":"A. Bandura. 2001. Social cognitive theory: an agentic perspective. Annual Review of Psychology 52: 1--26. http://doi.org/10.1146/annurev.psych.52.1.1","paperID":"None"},{"content":"Albert Bandura. 1977. Social learning theory. Prentice-Hall, Oxford, England.","paperID":"None"},{"content":"Albert Bandura. 2001. Social Cognitive Theory of Mass Communication. Media Psychology 3, 3: 265--299. http://doi.org/10.1207/S1532785XMEP0303_03","paperID":"None"},{"content":"Michael S. Bernstein, Greg Little, Robert C. Miller, et al. 2010. Soylent: a word processor with a crowd inside. Proceedings of the 23nd annual ACM symposium on User interface software and technology, ACM, 313--322. http://doi.org/10.1145/1866029.1866078","paperID":"None"},{"content":"W. Black and D. Wiliam. 1998. Assessment and classroom learning. Assessment in Education: Principles, Policy & Practice 5, 1.","paperID":"None"},{"content":"Michelene T. H. Chi. 2009. Active-ConstructiveInteractive: A Conceptual Framework for Differentiating Learning Activities. Topics in Cognitive Science 1, 1: 73--105. http://doi.org/10.1111/j.1756--8765.2008.01005.x","paperID":"None"},{"content":"Kwangsu Cho and Charles MacArthur. 2010. Student revision with peer and expert reviewing. Learning and Instruction 20, 4: 328--338. http://doi.org/10.1016/j.learninstruc.2009.08.006","paperID":"None"},{"content":"Steven Dow, Anand Kulkarni, Scott Klemmer, and Björn Hartmann. 2012. Shepherding the crowd yields better work. Proceedings of the ACM 2012 conference on Computer Supported Cooperative Work, 1013--1022. Retrieved July 12, 2013 from http://dl.acm.org/citation.cfm?id=2145355","paperID":"None"},{"content":"Ethan Fast, Daniel Steffee, Lucy Wang, Joel R. Brandt, and Michael S. Bernstein. 2014. Emergent, Crowdscale Programming Practice in the IDE. Proceedings of the 32Nd Annual ACM Conference on Human Factors in Computing Systems, ACM, 2491--2500. http://doi.org/10.1145/2556288.2556998","paperID":"None"},{"content":"Sarah Gielen, Elien Peeters, Filip Dochy, Patrick Onghena, and Katrien Struyven. 2010. Improving the effectiveness of peer feedback for learning. Learning and Instruction 20, 4: 304--315. http://doi.org/10.1016/j.learninstruc.2009.08.007","paperID":"None"},{"content":"Aniket Kittur, Jeffrey V. Nickerson, Michael Bernstein, et al. 2013. The future of crowd work. Proceedings of the 2013 conference on Computer supported cooperative work, ACM, 1301--1318. http://doi.org/10.1145/2441776.2441923","paperID":"None"},{"content":"Tak Yeon Lee, Casey Dugan, Werner Geyer, et al. 2013. Experiments on Motivational Feedback for Crowdsourced Workers. Seventh International AAAI Conference on Weblogs and Social Media. Retrieved October 25, 2013 from http://www.aaai.org/ocs/index.php/ICWSM/ICWSM13 /paper/view/6118","paperID":"None"},{"content":"Kurt Luther, Jari-Lee Tolentino, Wei Wu, et al. 2015. Structuring, Aggregating, and Evaluating Crowdsourced Design Critique. Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing, ACM, 473--485. http://doi.org/10.1145/2675133.2675283","paperID":"None"},{"content":"Ference Marton, Dai Hounsell, and Noel James Entwistle. 1997. The Experience of Learning: Implications for Teaching and Studying in Higher Education. Scottish Academic Press.","paperID":"None"},{"content":"Jon Noronha, Eric Hysen, Haoqi Zhang, and Krzysztof Z. Gajos. 2011. Platemate: crowdsourcing nutritional analysis from food photographs. Proceedings of the 24th annual ACM symposium on User interface software and technology, ACM, 1--12. http://doi.org/10.1145/2047196.2047198","paperID":"None"},{"content":"S.G. Paris and A.H. Paris. 2001. Classroom Applications of Research on Self-Regulated learning. Educational psychologist 36, 2.","paperID":"None"},{"content":"D. Royce Sadler. 1989. Formative assessment and the design of instructional systems. Instructional Science 18, 2: 119--144. http://doi.org/10.1007/BF00117714","paperID":"None"},{"content":"D. Schaster, D. Gilbert, D. Wegner, and M. Nock. 2014. Psychology. Worth Publishers, New York, NY.","paperID":"None"},{"content":"Jan-Willem Strijbos, Susanne Narciss, and Katrin Dünnebier. 2010. Peer feedback content and sender's competence level in academic writing revision tasks: Are they critical for feedback perceptions and efficiency? Learning and Instruction 20, 4: 291--303. http://doi.org/10.1016/j.learninstruc.2009.08.008","paperID":"None"},{"content":"Haiyi Zhu, Steven P. Dow, Robert E. Kraut, and Aniket Kittur. 2014. Reviewing Versus Doing: Learning and Performance in Crowd Assessment. Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing, ACM, 1445--1455. http://doi.org/10.1145/2531602.2531718","paperID":"None"},{"content":"Guidelines for Academic Requesters - WeAreDynamo Wiki. Retrieved September 22, 2014 from http://wiki.wearedynamo.org/index.php/Guidelines_for _Academic_Requesters","paperID":"None"}],"title":"Learning From the Crowd: Observational Learning in Crowdsourcing Communities","filename":"CHI16/p2635","authors":["Lena Mamykina","Thomas N. Smyth","Jill P. Dimond","Krzysztof Z. Gajos"],"conference":"CHI '16"}