{"paperId":2470775,"citation":[{"content":"Yanxia Zhang , Andreas Bulling , Hans Gellersen, Pupil-canthi-ratio: a calibration-free method for tracking horizontal gaze direction, Proceedings of the 2014 International Working Conference on Advanced Visual Interfaces, May 27-29, 2014, Como, Italy","paperID":"2598186"},{"content":"Karoliina Käki , Päivi Majaranta , Oleg Špakov , Jari Kangas, Effects of haptic feedback on gaze based auto scrolling, Proceedings of the 8th Nordic Conference on Human-Computer Interaction: Fun, Fast, Foundational, October 26-30, 2014, Helsinki, Finland","paperID":"2670247"},{"content":"Robert Walter , Andreas Bulling , David Lindlbauer , Martin Schuessler , Jörg Müller, Analyzing visual attention during whole body interaction with public displays, Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing, September 07-11, 2015, Osaka, Japan","paperID":"2804255"},{"content":"Jayson Turner , Shamsi Iqbal , Susan Dumais, Understanding gaze and scrolling strategies in text consumption tasks, Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers, September 07-11, 2015, Osaka, Japan","paperID":"2804331"},{"content":"John Paulin Hansen , Florian Biermann , Emilie Møllenbach , Haakon Lund , Javier San Agustin , Sebastian Sztuk, A GazeWatch Prototype, Proceedings of the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct, August 24-27, 2015, Copenhagen, Denmark","paperID":"2792899"},{"content":"Peter Kiefer , Yanxia Zhang , Andreas Bulling, The 5th international workshop on pervasive eye tracking and mobile eye-based interaction, Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers, September 07-11, 2015, Osaka, Japan","paperID":"2807960"},{"content":"Yanxia Zhang , Jörg Müller , Ming Ki Chong , Andreas Bulling , Hans Gellersen, GazeHorizon: enabling passers-by to interact with public displays by gaze, Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing, September 13-17, 2014, Seattle, Washington","paperID":"2636071"},{"content":"Mohamed Khamis , Florian Alt , Andreas Bulling, A field study on spontaneous gaze-based interaction with a public display using pursuits, Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers, September 07-11, 2015, Osaka, Japan","paperID":"2804335"},{"content":"Andreas Bulling, Human visual behaviour for collaborative human-machine interaction, Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers, September 07-11, 2015, Osaka, Japan","paperID":"2815378"},{"content":"Mélodie Vidal , Andreas Bulling , Hans Gellersen, Pursuits: spontaneous interaction with displays based on smooth pursuit eye movement and moving targets, Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing, September 08-12, 2013, Zurich, Switzerland","paperID":"2493477"},{"content":"Ken Pfeuffer , Melodie Vidal , Jayson Turner , Andreas Bulling , Hans Gellersen, Pursuit calibration: making gaze calibration less tedious and more flexible, Proceedings of the 26th annual ACM symposium on User interface software and technology, October 08-11, 2013, St. Andrews, Scotland, United Kingdom","paperID":"2501998"},{"content":"Christian Lander , Sven Gehring , Antonio Krüger , Sebastian Boring , Andreas Bulling, GazeProjector: Accurate Gaze Estimation and Seamless Gaze Interaction Across Multiple Displays, Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology, November 08-11, 2015, Daegu, Kyungpook, Republic of Korea","paperID":"2807479"},{"content":"Augusto Esteves , Eduardo Velloso , Andreas Bulling , Hans Gellersen, Orbits: Gaze Interaction for Smart Watches using Smooth Pursuit Eye Movements, Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology, November 08-11, 2015, Daegu, Kyungpook, Republic of Korea","paperID":"2807499"},{"content":"Marcus Carter , Joshua Newn , Eduardo Velloso , Frank Vetere, Remote Gaze and Gesture Tracking on the Microsoft Kinect: Investigating the Role of Feedback, Proceedings of the Annual Meeting of the Australian Special Interest Group for Computer Human Interaction, December 07-10, 2015, Parkville, VIC, Australia","paperID":"2838778"}],"reference":[{"content":"Brignull, H., and Rogers, Y. Enticing people to interact with large public displays in public spaces. In Proc. INTERACT 2003, IOS Press (2003), 17--24.","paperID":"None"},{"content":"Andrew T. Duchowski, Eye Tracking Methodology: Theory and Practice, Springer-Verlag New York, Inc., Secaucus, NJ, 2003","paperID":"640601"},{"content":"Marc Eaddy , Gabor Blasko , Jason Babcock , Steven Feiner, My Own Private Kiosk: Privacy-Preserving Public Displays, Proceedings of the Eighth International Symposium on Wearable Computers, p.132-135, October 31-November 03, 2004","paperID":"1033879"},{"content":"Jan-Mark Geusebroek , Rein van den Boomgaard , Arnold W.M. Smeulders , Hugo Geerts, Color Invariance, IEEE Transactions on Pattern Analysis and Machine Intelligence, v.23 n.12, p.1338-1350, December 2001","paperID":"507490"},{"content":"Dan Witzner Hansen , Qiang Ji, In the Eye of the Beholder: A Survey of Models for Eyes and Gaze, IEEE Transactions on Pattern Analysis and Machine Intelligence, v.32 n.3, p.478-500, March 2010","paperID":"1729561"},{"content":"Hutchinson, T., White, K. P. J., Martin, W., Reichert, K., and Frey, L. Human-computer interaction using eye-gaze input. Trans. Sys. Man Cyber Part C 19, 6 (1989), 1527--1534.","paperID":"None"},{"content":"Robert J. K. Jacob, The use of eye movements in human-computer interaction techniques: what you look at is what you get, ACM Transactions on Information Systems (TOIS), v.9 n.2, p.152-169, April 1991","paperID":"128728"},{"content":"Manu Kumar , Terry Winograd, Gaze-enhanced scrolling techniques, Proceedings of the 20th annual ACM symposium on User interface software and technology, October 07-10, 2007, Newport, Rhode Island, USA","paperID":"1294249"},{"content":"J. J. Magee , M. Betke , J. Gips , M. R. Scott , B. N. Waber, A Human–Computer Interface Using Symmetry Between Eyes to Detect Gaze Direction, IEEE Transactions on Systems, Man, and Cybernetics, Part A: Systems and Humans, v.38 n.6, p.1248-1261, November 2008","paperID":"2230395"},{"content":"Carlos H. Morimoto , Marcio R. M. Mimica, Eye gaze tracking techniques for interactive applications, Computer Vision and Image Understanding, v.98 n.1, p.4-24, April 2005","paperID":"1649095"},{"content":"Omar Mubin , Tatiana Lashina , Evert Loenen, How Not to Become a Buffoon in Front of a Shop Window: A Solution Allowing Natural Head Movement for Interaction with a Public Display, Proceedings of the 12th IFIP TC 13 International Conference on Human-Computer Interaction: Part II, August 24-28, 2009, Uppsala, Sweden","paperID":"1616258"},{"content":"Jörg Müller , Robert Walter , Gilles Bailly , Michael Nischt , Florian Alt, Looking glass: a field study on noticing interactivity of a shop window, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, May 05-10, 2012, Austin, Texas, USA","paperID":"2207718"},{"content":"Yasuto Nakanishi , Takashi Fujii , Kotaro Kiatjima , Yoichi Sato , Hideki Koike, Vision-Based Face Tracking System for Large Displays, Proceedings of the 4th international conference on Ubiquitous Computing, p.152-159, September 29-October 01, 2002, Göteborg, Sweden","paperID":"741495"},{"content":"Noris, B., Benmachiche, K., and Billard, A. Calibration-free eye gaze direction detection with gaussian processes. In Proc. VISAPP 2008, INSTICC (2008), 611--616.","paperID":"None"},{"content":"Peter Peltonen , Esko Kurvinen , Antti Salovaara , Giulio Jacucci , Tommi Ilmonen , John Evans , Antti Oulasvirta , Petri Saarikko, It's Mine, Don't Touch!: interactions at a large multi-touch display in a city centre, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, April 05-10, 2008, Florence, Italy","paperID":"1357255"},{"content":"Javier San Agustin , John Paulin Hansen , Martin Tall, Gaze-based interaction with public displays using off-the-shelf components, Proceedings of the 12th ACM international conference adjunct papers on Ubiquitous computing - Adjunct, September 26-29, 2010, Copenhagen, Denmark","paperID":"1864444"},{"content":"Laura Sesma , Arantxa Villanueva , Rafael Cabeza, Evaluation of pupil center-eye corner vector for gaze estimation using a web cam, Proceedings of the Symposium on Eye Tracking Research and Applications, March 28-30, 2012, Santa Barbara, California","paperID":"2168598"},{"content":"Linda E. Sibert , Robert J. K. Jacob, Evaluation of eye gaze interaction, Proceedings of the SIGCHI conference on Human Factors in Computing Systems, p.281-288, April 01-06, 2000, The Hague, The Netherlands","paperID":"332445"},{"content":"Andreas Sippl , Clemens Holzmann , Doris Zachhuber , Alois Ferscha, Real-time gaze tracking for public displays, Proceedings of the First international joint conference on Ambient intelligence, p.167-176, November 10-12, 2010, Malaga, Spain","paperID":"1926760"},{"content":"John D. Smith , Roel Vertegaal , Changuk Sohn, ViewPointer: lightweight calibration-free eye tracking for ubiquitous handsfree deixis, Proceedings of the 18th annual ACM symposium on User interface software and technology, October 23-26, 2005, Seattle, WA, USA","paperID":"1095043"},{"content":"Sugano, Y., Matsushita, Y., and Sato, Y. Calibration-free gaze sensing using saliency maps. In Proc. CVPR 2010, IEEE Computer Society (2010), 2667--2674.","paperID":"None"},{"content":"Valenti, R., and Gevers, T. Accurate eye center location and tracking using isophote curvature. In Proc. CVPR 2008, IEEE Computer Society (2008), 1--8.","paperID":"None"},{"content":"Roel Vertegaal , Aadil Mamuji , Changuk Sohn , Daniel Cheng, Media eyepliances: using eye tracking for remote control focus selection of appliances, CHI '05 Extended Abstracts on Human Factors in Computing Systems, April 02-07, 2005, Portland, OR, USA","paperID":"1057041"},{"content":"Vertegaal, R., Shell, J. S., Chen, D., and Mamuji, A. Designing for augmented attention: Towards a framework for attentive user interfaces. Computers in Human Behavior 22, 4 (2006), 771--789.","paperID":"None"},{"content":"Viola, P., and Jones, M. Rapid object detection using a boosted cascade of simple features. In Proc. CVPR 2001, IEEE Computer Society (2001), 511--518.","paperID":"None"},{"content":"Ward, D. J., and MacKay, D. J. C. Fast hands-free writing by gaze direction. Nature 418, 6900 (2002), 838.","paperID":"None"},{"content":"Yanxia Zhang , Andreas Bulling , Hans Gellersen, Towards pervasive eye tracking using low-level image features, Proceedings of the Symposium on Eye Tracking Research and Applications, March 28-30, 2012, Santa Barbara, California","paperID":"2168611"},{"content":"Zhiwei Zhu , Qiang Ji, Eye and gaze tracking for interactive graphic display, Machine Vision and Applications, v.15 n.3, p.139-148, July 2004","paperID":"1039455"},{"content":"Zhiwei Zhu , Qiang Ji, Eye Gaze Tracking under Natural Head Movements, Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Volume 1, p.918-923, June 20-26, 2005","paperID":"1069011"}],"abstract":"Eye gaze is compelling for interaction with situated displays as we naturally use our eyes to engage with them. In this work we present SideWays, a novel person-independent eye gaze interface that supports spontaneous interaction with displays: users can just walk up to a display and immediately interact using their eyes, without any prior user calibration or training. Requiring only a single off-the-shelf camera and lightweight image processing, SideWays robustly detects whether users attend to the centre of the display or cast glances to the left or right. The system supports an interaction model in which attention to the central display is the default state, while \"sidelong glances\" trigger input or actions. The robustness of the system and usability of the interaction model are validated in a study with 14 participants. Analysis of the participants' strategies in performing different tasks provides insights on gaze control strategies for design of SideWays applications.","video":"http://www.youtube.com/embed/nq2kd0zrXq8?rel=0","title":"SideWays: a gaze interface for spontaneous interaction with situated displays","filename":"CHI13/p851","authors":["Yanxia Zhang","Andreas Bulling","Hans Gellersen"],"conference":"CHI '13"}