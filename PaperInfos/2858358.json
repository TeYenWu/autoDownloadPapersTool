{"paperId":2858358,"citation":[],"reference":[{"content":"Daniel Ashbrook and Thad Starner. 2010. MAGIC: a motion gesture design tool. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 10), 2159--2168.","paperID":"None"},{"content":"Mehmet Aydın Baytaş, Yücel Yemez, and Oğuzhan Özcan. 2014. Hotspotizer: end-user authoring of mid-air gestural interactions. In Proceedings of the Nordic Conference on Human-Computer Interaction: Fun, Fast, Foundational (NordiCHI 14), 677--686.","paperID":"None"},{"content":"Ari Y. Benbasat, and Joseph A. Paradiso. 2002. An inertial measurement framework for gesture recognition and applications. In Gesture and Sign Language in Human-Computer Interaction, Ipke Wachsmuth and Timo Sowa (eds.). Springer, 9--20.","paperID":"None"},{"content":"Liwei Chan, Chi-Hao Hsieh, Yi-Ling Chen, Shuo Yang, Da-Yuan Huang, Rong-Hao Liang, and Bing-Yu Chen. 2015. Cyclops: Wearable and Single-Piece Full-Body Gesture Input Devices. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '15). 3001--3009.","paperID":"None"},{"content":"Xiang 'Anthony' Chen, Tovi Grossman, Daniel J. Wigdor, and George Fitzmaurice. 2014. Duet: exploring joint interactions on a smart phone and a smart watch. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '14). 159--168.","paperID":"None"},{"content":"Xiang 'Anthony' Chen, Nicolai Marquardt, Anthony Tang, Sebastian Boring, and Saul Greenberg. 2012. Extending a mobile device's interaction space through body-centric interaction. In Proceedings of the international conference on Human-computer interaction with mobile devices and services (MobileHCI '12). 151--160.","paperID":"None"},{"content":"Pei-Yu (Peggy) Chi and Yang Li. 2015. Weave: Scripting Cross-Device Wearable Interaction. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '15). 3923--3932.","paperID":"None"},{"content":"Andrew Crossan, John Williamson, Stephen Brewster, and Rod Murray-Smith. 2008. Wrist rotation for interaction in mobile contexts. In Proceedings of the international conference on Human computer interaction with mobile devices and services (MobileHCI '08). 435--438.","paperID":"None"},{"content":"Allen Cypher. 1993. Watch what I do: programming by demonstration. MIT Press.","paperID":"None"},{"content":"Tovi Grossman, Xiang Anthony Chen, and George Fitzmaurice. 2015. Typing on Glasses: Adapting Text Entry to Smart Eyewear. In Proceedings of the International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI '15). 144--152.","paperID":"None"},{"content":"Björn Hartmann, Leith Abdulla, Manas Mittal, and Scott R. Klemmer. 2007. Authoring sensor-based interactions by demonstration with direct manipulation and pattern recognition. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '07). 145--154.","paperID":"None"},{"content":"Lode Hoste, Brecht De Rooms, and Beat Signer. 2013. Declarative Gesture Spotting Using Inferred and Refined Control Points. In Proceedings of the International Conference on Pattern Recognition Applications and Methods (ICPRAM 2013).","paperID":"None"},{"content":"Steven Houben and Nicolai Marquardt. 2015. WatchConnect: A Toolkit for Prototyping Smartwatch Centric Cross-Device Applications. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '15). 1247--1256.","paperID":"None"},{"content":"Edwin L. Hutchins, James D. Hollan, and Donald A. Norman. 1985. Direct manipulation interfaces. Human-Computer Interaction, 1, 4: 311--338.","paperID":"None"},{"content":"Ideum. 2013. CreativeML. Retrieved September, 2015 from http://www.creativeml.org","paperID":"None"},{"content":"Ideum. 2014. Gesture Works. Retrieved September, 2015 from http://gestureworks.com","paperID":"None"},{"content":"Ju-Whan Kim and Tek-Jin Nam. 2013. EventHurdle: supporting designers' exploratory interaction prototyping with gesture-based sensors. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '13). 267--276.","paperID":"None"},{"content":"Kenrick Kin, Björn Hartmann, Tony DeRose, and Maneesh Agrawala. 2012. Proton: multitouch gestures as regular expressions. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '12). 2885--2894.","paperID":"None"},{"content":"Kenrick Kin, Björn Hartmann, Tony DeRose, and Maneesh Agrawala. 2012. Proton++: a customizable declarative multitouch framework. In Proceedings of the ACM symposium on User interface software and technology (UIST '12). 477--486.","paperID":"None"},{"content":"Sven Kratz and Michael Rohs. 2010. A $3 gesture recognizer: simple gesture recognition for devices equipped with 3D acceleration sensors. In Proceedings of the international conference on Intelligent user interfaces (IUI '10). 341--344.","paperID":"None"},{"content":"Yang Li. 2010. Protractor: a fast and accurate gesture recognizer. In Proceesdings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '10). 2169--2172.","paperID":"None"},{"content":"Jiayang Liu, Zhen Wang, Lin Zhong, Jehan Wickramasuriya, and Venu Vasudevan. 2009. uWave: Accelerometer-based personalized gesture recognition and its applications. In Proceedings of IEEE International Conference on Pervasive Computing and Communications (PerCom '09), 1--9.","paperID":"None"},{"content":"Hao Lü and Yang Li. 2012. Gesture coder: a tool for programming multi-touch gestures by demonstration. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '12). 2875--2884.","paperID":"None"},{"content":"Hao Lü and Yang Li. 2013. Gesture studio: authoring multi-touch interactions through demonstration and declaration. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '13). 257--266.","paperID":"None"},{"content":"Sushmita Mitra and Tinku Acharya. 2007. Gesture recognition: A survey. IEEE Transactions on Systems, Man, and Cybernetics-Part C: Applications and Reviews, 37, 3: 311--324.","paperID":"None"},{"content":"Miguel A. Nacenta, Yemliha Kamber, Yizhou Qiang, and Per Ola Kristensson. 2013. Memorability of pre-designed and user-defined gesture sets. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '13). 1099--1108.","paperID":"None"},{"content":"Donald A. Norman and Stephen W. Draper. 1986. User Centered System Design; New Perspectives on Human Computer Interaction. L. Erlbaum Assoc. Inc.","paperID":"None"},{"content":"Taiwoo Park, Jinwon Lee, Inseok Hwang, Chungkuk Yoo, Lama Nachman, and Junehwa Song. 2011. EGesture: a collaborative architecture for energy-efficient gesture recognition with hand-worn sensor and mobile devices. In Proceedings of the ACM Conference on Embedded Networked Sensor Systems (SenSys '11). 260--273.","paperID":"None"},{"content":"Dean Rubine. 1991. Specifying gestures by example. SIGGRAPH Comput. Graph. 25, 4: 329--337.","paperID":"None"},{"content":"Jaime Ruiz, Yang Li, and Edward Lank. 2011. User defined motion gestures for mobile interaction. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '11). 197--206.","paperID":"None"},{"content":"Hiroaki Sakoe and Seibi Chiba. 1978. Dynamic programming algorithm optimization for spoken word recognition. IEEE Transactions on Acoustics, Speech and Signal Processing, 26, 1: 43--49.","paperID":"None"},{"content":"Lucio Davide Spano, Antonio Cisternino, Fabio Paternò, and Gianni Fenu. 2013. GestIT: a declarative and compositional framework for multi-platform gesture definition. In Proceedings of the SIGCHI symposium on Engineering interactive computing systems (EICS '13). 187--196.","paperID":"None"},{"content":"Jacob O. Wobbrock, Meredith Ringel Morris, and Andrew D. Wilson. 2009. User-defined gestures for surface computing. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '09). 1083--1092.","paperID":"None"},{"content":"Jacob O. Wobbrock, Andrew D. Wilson, and Yang Li. 2007. Gestures without libraries, toolkits or training: a $1 recognizer for user interface prototypes. In Proceedings of the ACM symposium on User interface software and technology (UIST '07). 159--168.","paperID":"None"}],"abstract":"Gesture-based interaction is still underutilized in the mobile context despite the large amount of attention it has been given. Using accelerometers that are widely available in mobile devices, we developed M.Gesture, a software system that supports accelerometer-based gesture authoring on single or multiple mobile devices. The development was based on a formative study that showed users' preferences for subtle, simple motions and synchronized, multi-device gestures. M.Gesture adopts an acceleration data space and interface components based on mass-spring analogy and combines the strengths of both demonstration-based and declarative approaches. Also, gesture declaration is done by specifying a mass-spring trajectory with planes in the acceleration space. For iterative gesture modification, multi-level feedbacks are provided as well. The results of evaluative studies have shown good usability and higher recognition performance than that of dynamic time warping for simple gesture authoring. Later, we discuss the benefits of applying a physical metaphor and hybrid approach.","title":"M.Gesture: An Acceleration-Based Gesture Authoring System on Multiple Handheld and Wearable Devices","filename":"CHI16/p2307","authors":["Ju-Whan Kim","Han-Jong Kim","Tek-Jin Nam"],"conference":"CHI '16"}