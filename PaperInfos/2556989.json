{"paperId":2556989,"citation":[{"content":"Hana Vrzakova , Roman Bednarik , Yukiko I. Nakano , Fumio Nihei, Speakers' head and gaze dynamics weakly correlate in group conversation, Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research & Applications, March 14-17, 2016, Charleston, South Carolina","paperID":"2857522"}],"reference":[{"content":"Stylianos Asteriadis , Kostas Karpouzis , Stefanos Kollias, Feature Extraction and Selection for Inferring User Engagement in an HCI Environment, Proceedings of the 13th International Conference on Human-Computer Interaction. Part I: New Trends, July 19-24, 2009, San Diego, CA","paperID":"1601057"},{"content":"Bianchi-Berthouze, N. What Can Body Movement Tell Us About Players' Engagement? Measuring Behavior'12, ACM Press (2012), 94--97.","paperID":"None"},{"content":"Bianchi-Berthouze, N. Understanding the role of body movement in player engagement. Human Computer Interaction 28, 2 (2013), 42--75.","paperID":"None"},{"content":"Dan Bohus , Eric Horvitz, Learning to predict engagement with a spoken dialog system in open-world settings, Proceedings of the SIGDIAL 2009 Conference: The 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue, p.244-252, September 11-12, 2009, London, United Kingdom","paperID":"1708411"},{"content":"Dan Bohus , Eric Horvitz, Dialog in the open world: platform and applications, Proceedings of the 2009 international conference on Multimodal interfaces, November 02-04, 2009, Cambridge, Massachusetts, USA","paperID":"1647323"},{"content":"Brown, R.G. Exponential Smoothing for Predicting Demand. Little, 1956.","paperID":"None"},{"content":"Yoav Freund , Robert E. Schapire, A decision-theoretic generalization of on-line learning and an application to boosting, Journal of Computer and System Sciences, v.55 n.1, p.119-139, Aug. 1997","paperID":"261549"},{"content":"Harmonix. Dance Central. 2012. http://www.dancecentral.com.","paperID":"None"},{"content":"Björn Hartmann , Meredith Ringel Morris , Hrvoje Benko , Andrew D. Wilson, Pictionaire: supporting collaborative design work by integrating physical and digital artifacts, Proceedings of the 2010 ACM conference on Computer supported cooperative work, February 06-10, 2010, Savannah, Georgia, USA","paperID":"1718989"},{"content":"Ken Hinckley , Randy Pausch , John C. Goble , Neal F. Kassell, A survey of design issues in spatial input, Proceedings of the 7th annual ACM symposium on User interface software and technology, p.213-222, November 02-04, 1994, Marina del Rey, California, USA","paperID":"192501"},{"content":"Probabilistic Combination of Multiple Modalities to Detect Interest, Proceedings of the Pattern Recognition, 17th International Conference on (ICPR'04) Volume 3, p.969-972, August 23-26, 2004","paperID":"1021132"},{"content":"Rick Kjeldsen , Jacob Hartman, Design issues for vision-based computer interaction systems, Proceedings of the 2001 workshop on Perceptive user interfaces, November 15-16, 2001, Orlando, Florida","paperID":"971511"},{"content":"Andrea Kleinsmith , Nadia Bianchi-Berthouze, Affective Body Expression Perception and Recognition: A Survey, IEEE Transactions on Affective Computing, v.4 n.1, p.15-33, January 2013","paperID":"2498515"},{"content":"Michalowski, M.P., Sabanovic, S., and Simmons, R. A spatial model of engagement for a social robot. Advanced Motion Control'06, IEEE Computer Society Press (2006), 762--767.","paperID":"None"},{"content":"Microsoft. Xbox 360 + Kinect. http://www.xbox.com/kinect.","paperID":"None"},{"content":"Mota, S. and Picard, R. Automated posture analysis for detecting learner's interest level. In CVPR Workshop on HCI, (2003).","paperID":"None"},{"content":"Yukiko I. Nakano , Ryo Ishii, Estimating user's engagement from eye-gaze behaviors in human-agent conversations, Proceedings of the 15th international conference on Intelligent user interfaces, February 07-10, 2010, Hong Kong, China","paperID":"1719990"},{"content":"Udai Singh Pawar , Joyojeet Pal , Rahul Gupta , Kentaro Toyama, Multiple mice for retention tasks in disadvantaged schools, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, April 28-May 03, 2007, San Jose, California, USA","paperID":"1240864"},{"content":"Rare. Kinect Sports. 2010. http://www.rareusa.com/games/kinect-sports.","paperID":"None"},{"content":"Charles Rich , Brett Ponsleur , Aaron Holroyd , Candace L. Sidner, Recognizing engagement in human-robot interaction, Proceedings of the 5th ACM/IEEE international conference on Human-robot interaction, March 02-05, 2010, Osaka, Japan","paperID":"1734580"},{"content":"Jyotirmay Sanghvi , Ginevra Castellano , Iolanda Leite , André Pereira , Peter W. McOwan , Ana Paiva, Automatic analysis of affective postures and body motion to detect engagement with a game companion, Proceedings of the 6th international conference on Human-robot interaction, March 06-09, 2011, Lausanne, Switzerland","paperID":"1957781"},{"content":"Julia Schwarz , Scott Hudson , Jennifer Mankoff , Andrew D. Wilson, A framework for robust and flexible handling of inputs with uncertainty, Proceedings of the 23nd annual ACM symposium on User interface software and technology, October 03-06, 2010, New York, New York, USA","paperID":"1866039"},{"content":"P. Smith , M. Shah , N. da Vitoria Lobo, Determining driver visual attention with one camera, IEEE Transactions on Intelligent Transportation Systems, v.4 n.4, p.205-218, December 2003","paperID":"2218694"},{"content":"Stødle, D., Hagen, T., Bjorndalen, J., and Anshus, O. Gesture-based, touch-free multi-user gaming on wallsized, high-resolution tiled displays. Journal of Virtual Reality and Broadcasting 5, 10 (2008), 1860--2037.","paperID":"None"},{"content":"Paul Viola , Michael J. Jones, Robust Real-Time Face Detection, International Journal of Computer Vision, v.57 n.2, p.137-154, May 2004","paperID":"966458"},{"content":"Robert Walter , Gilles Bailly , Jörg Müller, StrikeAPose: revealing mid-air gestures on public displays, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, April 27-May 02, 2013, Paris, France","paperID":"2470774"}],"abstract":"Vision-based interfaces, such as those made popular by the Microsoft Kinect, suffer from the Midas Touch problem: every user motion can be interpreted as an interaction. In response, we developed an algorithm that combines facial features, body pose and motion to approximate a user's intention to interact with the system. We show how this can be used to determine when to pay attention to a user's actions and when to ignore them. To demonstrate the value of our approach, we present results from a 30-person lab study conducted to compare four engagement algorithms in single and multi-user scenarios. We found that combining intention to interact with a 'raise an open hand in front of you' gesture yielded the best results. The latter approach offers a 12% improvement in accuracy and a 20% reduction in time to engage over a baseline 'wave to engage' gesture currently used on the Xbox 360.","video":"http://www.youtube.com/embed/M0_HH2nhRE0?rel=0","title":"Combining body pose, gaze, and gesture to determine intention to interact in vision-based interfaces","filename":"CHI14/p3443","authors":["Julia Schwarz","Charles Claudius Marais","Tommer Leyvand","Scott E. Hudson","Jennifer Mankoff"],"conference":"CHI '14"}