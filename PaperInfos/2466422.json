{"paperId":2466422,"citation":[{"content":"Jun Kato, Integrated visual representations for programming with real-world input and output, Proceedings of the adjunct publication of the 26th annual ACM symposium on User interface software and technology, October 08-11, 2013, St. Andrews, Scotland, United Kingdom","paperID":"2508476"},{"content":"Jun Kato , Takeo Igarashi, VisionSketch: integrated support for example-centric programming of image processing applications, Proceedings of the 2014 Graphics Interface Conference, May 07-09, 2014, Montreal, Quebec, Canada","paperID":"2619668"}],"reference":[{"content":"Billard, A., Calinon, S., Dillmann, R. and Schaal, S. Robot programming by demonstration. In Handbook of Robotics, Springer (2008), 1371--1394.","paperID":"None"},{"content":"Scott Davidoff , Nicolas Villar , Alex S. Taylor , Shahram Izadi, Mechanical hijacking: how robots can accelerate UbiComp deployments, Proceedings of the 13th international conference on Ubiquitous computing, September 17-21, 2011, Beijing, China","paperID":"2030148"},{"content":"Björn Hartmann , Scott R. Klemmer , Michael Bernstein , Leith Abdulla , Brandon Burr , Avi Robinson-Mosher , Jennifer Gee, Reflective physical prototyping through integrated design, test, and analysis, Proceedings of the 19th annual ACM symposium on User interface software and technology, October 15-18, 2006, Montreux, Switzerland","paperID":"1166300"},{"content":"Jun Kato , Sean McDirmid , Xiang Cao, DejaVu: integrated support for developing interactive camera-based programs, Proceedings of the 25th annual ACM symposium on User interface software and technology, October 07-10, 2012, Cambridge, Massachusetts, USA","paperID":"2380142"},{"content":"Andrew J. Ko , Brad A. Myers, Barista: An implementation framework for enabling new tools, interaction techniques and views in code editors, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, April 22-27, 2006, Montréal, Québec, Canada","paperID":"1124831"},{"content":"LEGO Mindstorms NXT. http://mindstorms.lego.com/.","paperID":"None"},{"content":"Nakaoka, S., Kajita, S. and Yokoi, K. Intuitive and flexible user interface for creating whole body motions of biped humanoid robots. In Proc. IROS 2010, 1675--1682.","paperID":"None"},{"content":"Cyrus Omar , YoungSeok Yoon , Thomas D. LaToza , Brad A. Myers, Active code completion, Proceedings of the 2012 International Conference on Software Engineering, June 02-09, 2012, Zurich, Switzerland","paperID":"2337324"},{"content":"Kayur Patel , Naomi Bancroft , Steven M. Drucker , James Fogarty , Andrew J. Ko , James Landay, Gestalt: integrated support for implementation and analysis in machine learning, Proceedings of the 23nd annual ACM symposium on User interface software and technology, October 03-06, 2010, New York, New York, USA","paperID":"1866038"},{"content":"Processing. http://processing.org/.","paperID":"None"},{"content":"Victor, B. Learnable Programming. http://worrydream.com/LearnableProgramming/.","paperID":"None"},{"content":"Tom Yeh , Tsung-Hsiang Chang , Robert C. Miller, Sikuli: using GUI screenshots for search and automation, Proceedings of the 22nd annual ACM symposium on User interface software and technology, October 04-07, 2009, Victoria, BC, Canada","paperID":"1622213"},{"content":"Wataru Yoshizaki , Yuta Sugiura , Albert C. Chiou , Sunao Hashimoto , Masahiko Inami , Takeo Igarashi , Yoshiaki Akazawa , Katsuaki Kawachi , Satoshi Kagami , Masaaki Mochimaru, An actuated physical puppet as an input device for controlling a digital manikin, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, May 07-12, 2011, Vancouver, BC, Canada","paperID":"1979034"}],"abstract":"Current programming environments use textual or symbolic representations. While these representations are appropriate for describing logical processes, they are not appropriate for representing raw values such as human and robot posture data, which are necessary for handling gesture input and controlling robots. To address this issue, we propose Picode, a text-based development environment integrated with visual representations: photos of human and robots. With Picode, the user first takes a photo to bind it to posture data. S/he then drag-and-drops the photo into the code editor, where it is displayed as an inline image. A preliminary in-house user study implied positive effects of taking photos on the programming experience.","video":"http://www.youtube.com/embed/NCou4Q1STE8?rel=0","title":"Picode: inline photos representing posture data in source code","filename":"CHI13/p3097","authors":["Jun Kato","Daisuke Sakamoto","Takeo Igarashi"],"conference":"CHI '13"}