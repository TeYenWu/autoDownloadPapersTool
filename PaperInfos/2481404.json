{"paperId":2481404,"citation":[{"content":"Nicola Dell , Trevor Perrier , Neha Kumar , Mitchell Lee , Rachel Powers , Gaetano Borriello, Paper-Digital Workflows in Global Development Organizations, Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing, March 14-18, 2015, Vancouver, BC, Canada","paperID":"2675145"},{"content":"Joshua E. Blumenstock , Niall Keleher, The Price is Right?: Statistical evaluation of a crowd-sourced market information system in Liberia, Proceedings of the 2015 Annual Symposium on Computing for Development, December 01-02, 2015, London, United Kingdom","paperID":"2830647"},{"content":"Ted McCarthy , Brian DeRenzi , Joshua Blumenstock , Emma Brunskill, Towards operationalizing outlier detection in community health programs, Proceedings of the Sixth International Conference on Information and Communications Technologies and Development: Notes, p.88-91, December 07-10, 2013, Cape Town, South Africa","paperID":"2517919"}],"reference":[{"content":"Baker, R. P. New technology in survey research: Computer-assisted personal interviewing (CAPI). Social Science: Computer Review 10, 2 (1992), 145--157.","paperID":"None"},{"content":"Bennett, A. Toward a solution of the \"cheater problem\" among part-time research investigators. J. Marketing 2 (1948), 470--474.","paperID":"None"},{"content":"Birnbaum, B. Algorithmic Approaches to Detecting Interviewer Fabrication in Surveys. PhD thesis, U. Washington, 2012.","paperID":"None"},{"content":"Benjamin Birnbaum , Brian DeRenzi , Abraham D. Flaxman , Neal Lesh, Automated quality control for mobile data collection, Proceedings of the 2nd ACM Symposium on Computing for Development, March 11-12, 2012, Atlanta, Georgia","paperID":"2160603"},{"content":"Blaya, J. A., et al. E-health technologies show promise in developing countries. Health Aff. (Millwood) 29, 2 (2010), 244--51.","paperID":"None"},{"content":"Bredl, S., et al. A statistical approach to detect cheating interviewers. Tech. Rep. 39, University Giessen, Center for International Development and Environmental Research (ZEU), 2008.","paperID":"None"},{"content":"Leo Breiman, Random Forests, Machine Learning, v.45 n.1, p.5-32, October 1 2001","paperID":"570182"},{"content":"Bushery, J. M., et al. Using date and time stamps to detect interviewer falsification. Proc. ASA (Survey Research Methods) (1999), 316--320.","paperID":"None"},{"content":"Rich Caruana , Nikos Karampatziakis , Ainur Yessenalina, An empirical evaluation of supervised learning in high dimensions, Proceedings of the 25th international conference on Machine learning, p.96-103, July 05-09, 2008, Helsinki, Finland","paperID":"1390169"},{"content":"Kuang Chen , Harr Chen , Neil Conway , Joseph M. Hellerstein , Tapan S. Parikh, Usher: Improving Data Quality with Dynamic Forms, IEEE Transactions on Knowledge and Data Engineering, v.23 n.8, p.1138-1153, August 2011","paperID":"2007016"},{"content":"Cho, M. J., et al. Inferential methods to identify possible interviewer fraud using leading digit preference patterns and design effect matrices. Proc. ASA (Survey Research Methods Section) (2003), 936--941.","paperID":"None"},{"content":"Mick P. Couper, Usability evaluation of computer-assisted survey instruments, Social Science Computer Review, v.18 n.4, p.384-396, Winter 2000","paperID":"366039"},{"content":"Couper, M. P., and Kreuter, F. Using paradata to explore item level response times in surveys. J. Royal Statistical Society: A (2012).","paperID":"None"},{"content":"Crespi, L. P. The cheater problem in polling. Public Opinion Quarterly 9, 4 (1945), 431--445.","paperID":"None"},{"content":"DeRenzi, B., et al. Mobile phone tools for field-based health care workers in low-income countries. Mt. Sinai J. Medicine 78, 3 (2011), 406--418.","paperID":"None"},{"content":"EpiSurveyor. http://www.episurveyor.org/.","paperID":"None"},{"content":"Evans, F. B. On interviewer cheating. Public Opinion Quarterly 25 (1961), 126--127.","paperID":"None"},{"content":"Arin Ghazarian , S. Majid Noorhosseini, Automatic detection of users' skill levels using high-frequency user interface events, User Modeling and User-Adapted Interaction, v.20 n.2, p.109-146, June      2010","paperID":"1825391"},{"content":"Mark Hall , Eibe Frank , Geoffrey Holmes , Bernhard Pfahringer , Peter Reutemann , Ian H. Witten, The WEKA data mining software: an update, ACM SIGKDD Explorations Newsletter, v.11 n.1, June 2009","paperID":"1656278"},{"content":"Hansen, S. E., and Marvin, T. Reporting on item times and keystrokes from Blaise audit trails. Tech. rep., 2001.","paperID":"None"},{"content":"Carl Hartung , Adam Lerer , Yaw Anokwa , Clint Tseng , Waylon Brunette , Gaetano Borriello, Open data kit: tools to build information services for developing regions, Proceedings of the 4th ACM/IEEE International Conference on Information and Communication Technologies and Development, p.1-12, December 13-16, 2010, London, United Kingdom","paperID":"2369236"},{"content":"David M. Hilbert , David F. Redmiles, Extracting usability information from user interface events, ACM Computing Surveys (CSUR), v.32 n.4, p.384-421, Dec. 2000","paperID":"371593"},{"content":"Hong, H. S., et al. Adoption of a PDA-based home hospice care system for cancer patients. Comput. Inform. Nurs. 27, 6 (2009), 365--71.","paperID":"None"},{"content":"Hood, C. C., and Bushery, J. M. Getting more bang from the reinterview buck: Identifying \"at risk\" interviewers. Proc. ASA (Survey Research Methods Section) (1997), 820--824.","paperID":"None"},{"content":"Amy Hurst , Scott E. Hudson , Jennifer Mankoff , Shari Trewin, Automatically detecting pointing performance, Proceedings of the 13th international conference on Intelligent user interfaces, January 13-16, 2008, Gran Canaria, Spain","paperID":"1378776"},{"content":"Inciardi, J. A. Fictitious data in drug abuse research. Intl. J. Addictions 16 (1981), 377--380.","paperID":"None"},{"content":"Judge, G., and Schechter, L. Detecting problems in survey data using Benford's Law. J. Human Resources 44, 1 (2009), 1--24.","paperID":"None"},{"content":"Kiecker, P., and Nelson, J. E. Do interviewers follow telephone survey instructions? J. Market Research Society 38 (1996), 161--176.","paperID":"None"},{"content":"Krejsa, E. A., et al. Evaluation of the quality assurance falsification interview used in the Census 2000 dress rehearsal. Proc. ASA (Survey Research Methods Section) (1999), 635--640.","paperID":"None"},{"content":"Lal, S. O., et al. Palm computer demonstrates a fast and accurate means of burn data collection. J. Burn Care Rehabil. 21, 6 (2000), 559--61.","paperID":"None"},{"content":"Li, J., et al. Using statistical models for sample design of a reinterview program. Proc. ASA (Survey Research Methods Section) (2009), 4681--4695.","paperID":"None"},{"content":"Murphy, J., et al. A system for detecting interview falsification. In American Assoc. Public Opinion Research 59th Ann. Conf. (2004).","paperID":"None"},{"content":"Tapan S. Parikh , Paul Javid , Sasikumar K. , Kaushik Ghosh , Kentaro Toyama, Mobile phones and paper documents: evaluating a new approach for capturing microfinance data in rural India, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, April 22-27, 2006, Montréal, Québec, Canada","paperID":"1124857"},{"content":"Pendragon Forms. http://pendragonsoftware.com/.","paperID":"None"},{"content":"Porras, J., and English, N. Data-driven approaches to identifying interviewer data falsification: The case of health surveys. Proc. ASA (Survey Research Methods Section) (2004), 4223--4228.","paperID":"None"},{"content":"Aishwarya Lakshmi Ratan , Kentaro Toyama , Sunandan Chakraborty , Keng Siang Ooi , Mike Koenig , Pushkar V. Chitnis , Matthew Phiong, Managing microfinance with paper, pen and digital slate, Proceedings of the 4th ACM/IEEE International Conference on Information and Communication Technologies and Development, p.1-11, December 13-16, 2010, London, United Kingdom","paperID":"2369255"},{"content":"Jeffrey M. Rzeszotarski , Aniket Kittur, Instrumenting the crowd: using implicit behavioral measures to predict task performance, Proceedings of the 24th annual ACM symposium on User interface software and technology, October 16-19, 2011, Santa Barbara, California, USA","paperID":"2047199"},{"content":"Schreiner, I., et al. Interviewer falsification in census bureau surveys. Proc. ASA (Survey Research Methods Section) (1988), 491--496.","paperID":"None"},{"content":"Shäfer, C., et al. Automatic identification of faked and fraudulent interviews in surveys by two different methods. Proc. ASA (Survey Research Methods Section) (2004), 4318--4325.","paperID":"None"},{"content":"Stefan Stieger , Ulf-Dietrich Reips, What are participants doing while filling in an online questionnaire: A paradata collection tool and an empirical study, Computers in Human Behavior, v.26 n.6, p.1488-1495, November, 2010","paperID":"1853507"},{"content":"Turner, C. F., et al. Falsification in epidemiologic surveys: Detection and remediation. Tech. Rep. 53, Research Triangle Institute, 2002.","paperID":"None"},{"content":"United Nations Dept. of Economic and Social Affairs, Population Division. World Urbanization Prospects, 2011.","paperID":"None"},{"content":"United Nations Development Programme. The Human Development Report, 2011.","paperID":"None"}],"abstract":"Surveys conducted by human interviewers are one of the principal means of gathering data from all over the world, but the quality of this data can be threatened by interviewer fabrication. In this paper, we investigate a new approach to detecting interviewer fabrication automatically. We instrument electronic data collection software to record logs of low-level behavioral data and show that supervised classification, when applied to features extracted from these logs, can identify interviewer fabrication with an accuracy of up to 96%. We show that even when interviewers know that our approach is being used, have some knowledge of how it works, and are incentivized to avoid detection, it can still achieve an accuracy of 86%. We also demonstrate the robustness of our approach to a moderate amount of label noise and provide practical recommendations, based on empirical evidence, on how much data is needed for our approach to be effective.","video":"http://www.youtube.com/embed/DU_2ppDALIw?rel=0","title":"Using behavioral data to identify interviewer fabrication in surveys","filename":"CHI13/p2911","authors":["Benjamin Birnbaum","Gaetano Borriello","Abraham D. Flaxman","Brian DeRenzi","Anna R. Karlin"],"conference":"CHI '13"}