{"paperId":2702296,"reference":[{"content":"Amazon's Mechanical Turk. (2014, January). Retrieved from Amazon's Mechanical Turk: https://www.mturk.com/mturk/","paperID":"None"},{"content":"Cechanowicz, J., Gutwin, C., Brownell, B., & Goodfellow, L. (2013). Effects of Gamification on Participation and Data Quality in a Real-World Market Research Domain. Gamification 2013, (pp. 58--65). Stratford, Ontario.","paperID":"None"},{"content":"Cleveland, W. S., & McGill, R. (1984). Graphical perception: Theory, experimentation, and application to the development of graphical methods. J. Am. Statistical Association, 79, 531--554.","paperID":"None"},{"content":"Cooper, S., Khatib, F., Treuille, A., Barbero, J., Lee, J., Beenen, M., et al. (2010). Predicting protein structures with a multiplayer online game. Nature, 756--760.","paperID":"None"},{"content":"de la Iglesia, J. L., & Gayo, J. E. (2009). Doing Business by Selling Free Services. In Web 2.0 (pp. 114).","paperID":"None"},{"content":"Deterding, S., Bjork, S., Nacke, L. E., Dixon, D., & Lawley, E. (2013). Designing Gamification: Creating Gameful and Playful Experiences. CHI 2013: (pp. 3263--3266). Paris, France: ACM.","paperID":"None"},{"content":"Deterding, S., Sicart, M., Nacke, L. E., O'Hara, K., & Dixon, D. (2011). Gamification: Using Game-Design Elements in Non-Gaming Contexts. CHI '11 Extended Abstracts (pp. 2425--2428)..","paperID":"None"},{"content":"Downs, J. S., Holbrook, M. B., Sheng, S., & Cranor, L. F. (2010). Are your participants gaming the system?: Screening Mechanical Turk workers. In Proc. CHI '10 (pp. 2399--2402). ACM.","paperID":"None"},{"content":"Eickhoff, C., Harris, C., & de Vries, A. P. (2012). Quality through Flow and Immersion: Gamifying Crowdsourced Relevance Assessments. SIGIR'12 (pp. 871--880). ACM.","paperID":"None"},{"content":"Fatla, D. R., Gutwin, C., Nacke, L. E., Bateman, S., & Mandryk, R. L. (2011). Calibration Games: Making Calibration Tasks Enjoyable by Adding Motivating Game Elements. UIST '11 (pp. 403--411).","paperID":"None"},{"content":"Guy, I., Perer, A., Daniel, T., Greenshpan, O., & Turbahn, I. (2011). Guess Who? Enriching the Social Graph through a Crowdsourcing Game. CHI 2011 (pp. 1373--1382). Vancouver, BC: ACM.","paperID":"None"},{"content":"Heer, J., & Bostock, M. (2010). Crowdsourcing Graphical Perception: Using Mechanical Turk to Asses Visualization Design. CHI 2010 (pp. 203--212).","paperID":"None"},{"content":"Komarov, S., Reinecke, K., & Gajos, K. Z. (2013). Crowdsourcing Performance Evaluations of User Interfaces. CHI 2013, Paris, France: ACM.","paperID":"None"},{"content":"Kraut, R., Olson, J., Banaji, M., Bruckman, A., Cohen, J., & Couper, M. (2004). Psychological Research Online: Opportunities and Challenges. American Psychologist, 105--117.","paperID":"None"},{"content":"MacKenzie, S. I. (1992). Fitts' Law as a Research and Design Tool in Human-Computer Interaction. HumanComputer Interaction, 7, 91--139.","paperID":"None"},{"content":"Mason, W., & Suri, S. (2012). Conducting Behavioural Research on Amazon's Mechanical Turk. Behavior Research Methods, 1--23.","paperID":"None"},{"content":"Reinecke, K., & Gajos, K. (n.d.). LabintheWild: Conducting Large-Scale Online Experiments With Uncompensated Samples. To appear in CSCW'15.","paperID":"None"},{"content":"von Ahn, L., & Dabbish, L. (2004). Labeling Images with a Computer Game. CHI 2004 (pp. 319--326). Vienna, Austria: ACM.","paperID":"None"},{"content":"von Ahn, L., & Dabbish, L. (2008). Designing Games With A Purpose. Communications of the ACM, 58--67.","paperID":"None"},{"content":"von Ahn, L., Ginosar, S., Kedia, M., Liu, R., & Blum, M. (2006). Improving Accessibility of the Web with a Computer Game. CHI 2006 (pp. 79--82). ACM.","paperID":"None"},{"content":"Watson, D., Hancock, M., & Mandryk, R. (2013). Gamifying Behaviour that Leads to Learning. Gamification '13, (pp. 87--90).","paperID":"None"}],"citation":[],"abstract":"Classic ways of gathering data on human behaviour are time-consuming, costly and are subject to limited participant pools. Crowdsourcing offers a reduction in operating costs and access to a diverse and large participant pool; however issues arise concerning low worker pay and questions about data quality. Gamification provides a motivation to participate, but also requires the development of specialized, research-question specific games that can be costly to produce. Our solution combines gamification and crowdsourcing in a smartphone-based system that emulates the popular Freemium model of play to motivate voluntary participation through in-game rewards, using a robust framework to study multiple unrelated research questions within the same system. We deployed our game on the Android store and compared it to a gamified laboratory version and a non-gamified laboratory version, and found that players who used the in-game rewards were motivated to do experimental tasks. There was no difference between the systems for performance on a motor task; however, performance on the cognitive task was worse for the crowdsourced game. We discuss options for improving performance on tasks requiring attention.","title":"Mobile Gamification for Crowdsourcing Data Collection: Leveraging the Freemium Model","filename":"CHI15/p1065","authors":["Kristen Dergousoff","Regan L. Mandryk"],"conference":"CHI '15"}