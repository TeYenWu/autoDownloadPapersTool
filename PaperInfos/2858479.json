{"paperId":2858479,"citation":[],"abstract":"We present a computational model to predict users' spatio-temporal visual attention on WIMP-style (windows, icons, menus, pointer) graphical user interfaces. Like existing models of bottom-up visual attention in computer vision, our model does not require any eye tracking equipment. Instead, it predicts attention solely using information available to the interface, specifically users' mouse and keyboard input as well as the UI components they interact with. To study our model in a principled way, we further introduce a method to synthesize user interface layouts that are functionally equivalent to real-world interfaces, such as from Gmail, Facebook, or GitHub. We first quantitatively analyze attention allocation and its correlation with user input and UI components using ground-truth gaze, mouse, and keyboard data of 18 participants performing a text editing task. We then show that our model predicts attention maps more accurately than state-of-the-art methods. Our results underline the significant potential of spatio-temporal attention modeling for user interface evaluation, optimization, or even simulation.","reference":[{"content":"E. Arroyo, T. Selker, and W. Wei. 2006. Usability tool for analysis of web designs using mouse tracks. In Ext. Abstr. CHI. 484--489. http://dx.doi.org/10.1145/1125451.1125557","paperID":"None"},{"content":"R. Atterer, M. Wnuk, and A. Schmidt. 2006. Knowing the user's every move: user activity tracking for website usability evaluation and implicit interaction. In Proc. WWW. 203--212. http://dx.doi.org/10.1145/1135777.1135811","paperID":"None"},{"content":"H. J. Bieg, H. Reiterer, and H. H. Bülthoff. 2010. Eye and pointer coordination in search and selection tasks. In Proc. ETRA. http://dx.doi.org/10.1145/1743666.1743688","paperID":"None"},{"content":"A. Borji. 2013. State-of-the-art in visual attention modeling. IEEE TPAMI (2013). http://dx.doi.org/10.1109/TPAMI.2012.89","paperID":"None"},{"content":"A. Borji, D. N. Sihite, and L. Itti. 2011. Computational modeling of top-down visual attention in interactive environments. In Proc. BMVC. 1--12. http://dx.doi.org/10.5244/C.25.85","paperID":"None"},{"content":"A. Borji, H. R. Tavakoli, D. N. Sihite, and L. Itti. 2013. Analysis of scores, datasets, and models in visual saliency prediction. ICCV (2013). http://dx.doi.org/10.1109/ICCV.2013.118","paperID":"None"},{"content":"N. Bruce and J. Tsotsos. 2006. Saliency based on information maximization. In Proc. NIPS.","paperID":"None"},{"content":"N. D. B. Bruce and J. K. Tsotsos. 2008. Spatiotemporal saliency: towards a hierarchical representation of visual saliency. Proc. WAPCV (2008). http://dx.doi.org/10.1007/978--3--642-00582--4_8","paperID":"None"},{"content":"A. Bulling. 2016. Pervasive Attentive User Interfaces. IEEE Computer 49, 1 (2016), 94--98. http://dx.doi.org/10.1109/MC.2016.32","paperID":"None"},{"content":"A. Bulling, F. Alt, and A. Schmidt. 2012. Increasing the Security of Gaze-Based Cued-Recall Graphical Passwords Using Saliency Masks. In Proc. CHI. 3011--3020. http://dx.doi.org/10.1145/2207676.2208712","paperID":"None"},{"content":"A. Bulling, C. Weichel, and H. Gellersen. 2013. EyeContext: Recognition of High-level Contextual Cues from Human Visual Behaviour. In Proc. CHI. 305--308. http://dx.doi.org/10.1145/2470654.2470697","paperID":"None"},{"content":"A. Bulling and T. O. Zander. 2014. Cognition-Aware Computing. IEEE Pervasive Computing 13, 3 (July 2014), 80--83. http://dx.doi.org/10.1109/mprv.2014.42","paperID":"None"},{"content":"M. C. Chen, J. R. Anderson, and M. H. Sohn. 2001. What can a mouse cursor tell us more? Correlation of eye/mouse movements on web browsing.. In Proc. CHI. http://dx.doi.org/10.1145/634067.634234","paperID":"None"},{"content":"F. Courtemanche, E. Aïmeur, A. Dufresne, M. Najjar, and F. Mpondo. 2011. Activity recognition using eye-gaze movements and traditional interactions. Interacting with Computers 23, 3 (2011), 202--213. http://dx.doi.org/10.1016/j.intcom.2011.02.008","paperID":"None"},{"content":"A. L. Cox and M. M. Silva. 2006. The role of mouse movements in interactive search. In Proc. CogSci. 1156--1161.","paperID":"None"},{"content":"K. Ehinger, B. Hidalgo-Sotelo, A. Torralba, and A. Oliva. 2009. Modeling search for people in 900 scenes. Visual Cognition (2009). http://dx.doi.org/10.1080/13506280902834720","paperID":"None"},{"content":"A. Elbahi, M. A. Mahjoub, and M. N. Omri. 2013. Hidden Markov model for inferring user task using mouse movement. In Proc. ICTA. 1--7. http://dx.doi.org/10.1109/ICTA.2013.6815305","paperID":"None"},{"content":"E. Erdem and A. Erdem. 2013. Visual saliency estimation by nonlinearly integrating features using region covariances. Journal of Vision (2013). http://dx.doi.org/10.1167/13.4.11","paperID":"None"},{"content":"J. E. Garrido, V. M. R. Penichet, M. D. Lozano, A. Quigley, and P. O. Kristensson. 2014. AwToolkit: attention-aware user interface widgets. In Proc. AVI (2014). http://dx.doi.org/10.1145/2598153.2598160","paperID":"None"},{"content":"Q. Guo and E. Agichtein. 2008. Exploring mouse movements for inferring query intent. In Proc. SIGIR. 707--708. http://dx.doi.org/10.1145/1390334.1390462","paperID":"None"},{"content":"Q. Guo and E. Agichtein. 2010. Towards predicting web searcher gaze position from mouse movements. In Ext. Abstr. CHI. 3601--3606. http://dx.doi.org/10.1145/1753846.1754025","paperID":"None"},{"content":"J. Harel, C. Koch, and P. Perona. 2007. Graph-based visual saliency. In Proc. NIPS.","paperID":"None"},{"content":"J. Huang, R. White, and G. Buscher. 2012. User see, user point: Gaze and cursor alignment in web search. In Proc. CHI. http://dx.doi.org/10.1145/2207676.2208591","paperID":"None"},{"content":"J. Huang, R. W. White, and S. Dumais. 2011. No clicks, no problem: using cursor movements to understand and improve search. In Proc. CHI. 1225--1234. http://dx.doi.org/10.1145/1978942.1979125","paperID":"None"},{"content":"L. Itti and C. Koch. 2001. Computational modelling of visual attention. Nature reviews neuroscience 2, 3 (2001), 194--203. http://dx.doi.org/10.1038/35058500","paperID":"None"},{"content":"L. Itti, C. Koch, and E. Niebur. 1998. A model of saliency based visual attention for rapid scene analysis. PAMI (1998). http://dx.doi.org/10.1109/34.730558","paperID":"None"},{"content":"R. J. K. Jacob. 1990. What you look at is what you get: eye movement-based interaction techniques. In Proc. CHI. 11--18. http://dx.doi.org/10.1145/97243.97246","paperID":"None"},{"content":"M. Jiang, S. Huang, J. Duan, and Q. Zhao. 2015. SALICON: Saliency in context. In Proc. CVPR. http://dx.doi.org/10.1109/CVPR.2015.7298710","paperID":"None"},{"content":"R. Johansson, Å. Wengelin, V. Johansson, and K. Holmqvist. 2010. Looking at the keyboard or the monitor: relationship with text production processes. Reading and writing 23, 7 (2010), 835--851. http://dx.doi.org/10.1007/s11145-009--9189--3","paperID":"None"},{"content":"T. Judd, K. Ehinger, F. Durand, and A. Torralba. 2009. Learning to predict where humans look. In Proc. ICCV. http://dx.doi.org/10.1109/ICCV.2009.5459462","paperID":"None"},{"content":"J. Li, Y. Tian, T. Huang, and Gao W. 2010. Probabilistic multi-task learning for visual saliency estimation in video. IJCV (2010). http://dx.doi.org/10.1007/s11263-010-0354--6","paperID":"None"},{"content":"D. J. Liebling and S. T. Dumais. 2014. Gaze and Mouse Coordination in Everyday Work. In Adjunct Proc. UbiComp '14. 1141--1150. http://dx.doi.org/10.1145/2638728.2641692","paperID":"None"},{"content":"P. P. Maglio, R. Barrett, C. S. Campbell, and T. Selker. 2000. SUITOR: An attentive information system. In Proc. IUI. 169--176. http://dx.doi.org/10.1145/325737.325821","paperID":"None"},{"content":"P. Majaranta and A. Bulling. 2014. Eye Tracking and Eye-Based Human-Computer Interaction. Springer Publishing, 39--65. http://dx.doi.org/10.1007/978--1--4471--6392--3_3","paperID":"None"},{"content":"P. Majaranta and K. J. Raiha. 2002. Twenty years of eye typing: systems and design issues. In Proc. ETRA. 15--22. http://dx.doi.org/10.1145/507072.507076","paperID":"None"},{"content":"C. M. Masciocchi and J. D. Still. 2013. Alternatives to Eye Tracking for Predicting Stimulus-Driven Attentional Selection Within Interfaces. Human-Computer Interaction 28, 5 (2013), 417--441. http://dx.doi.org/10.1080/07370024.2012.731332","paperID":"None"},{"content":"V. Navalpakkam and E. Churchill. 2012. Mouse Tracking: Measuring and Predicting Users' Experience of Web-based Content. In Proc. CHI. 2963--2972. http://dx.doi.org/10.1145/2207676.2208705","paperID":"None"},{"content":"V. Navalpakkam, L. Jentzsch, R. Sayres, S. Ravi, A. Ahmed, and A. Smola. 2013. Measurement and modeling of eye-mouse behavior in the presence of nonlinear page layouts. In Proc. WWW.","paperID":"None"},{"content":"R. J. Peters and L. Itti. 2007. Beyond bottom-up: Incorporating task-dependent influences into a computational model of spatial attention. In Proc. CVPR. http://dx.doi.org/10.1109/CVPR.2007.383337","paperID":"None"},{"content":"R. J. Peters, A. Iyer, L. Itti, and C. Koch. 2008. Objects predict fixations better than early saliency. Journal of Vision (2008). http://dx.doi.org/10.1167/8.14.18","paperID":"None"},{"content":"K. Rodden, X. Fu, A. Aula, and I. Spiro. 2008. Eye-mouse coordination patterns on web search results pages. In Ext. Abstr. CHI. 2997--3002. http://dx.doi.org/10.1145/1358628.1358797","paperID":"None"},{"content":"D. D. Salvucci and J. R. Anderson. 2000. Intelligent gaze-added interfaces. In Proc. CHI. 273--280. http://dx.doi.org/10.1145/332040.332444","paperID":"None"},{"content":"C. Shen and Q. Zhao. 2014. Webpage Saliency. In Proc. ECCV. 33--46. http://dx.doi.org/10.1007/978--3--319--10584-0_3","paperID":"None"},{"content":"L. E. Sibert and R. J. K. Jacob. 2000. Evaluation of eye gaze interaction. In Proc. CHI. 281--288. http://dx.doi.org/10.1145/332040.332445","paperID":"None"},{"content":"B. Smith, J. Ho, W. Ark, and S. Zhai. 2000. Hand eye coordination patterns in target selection. In Proc. ETRA. http://dx.doi.org/10.1145/355017.355041","paperID":"None"},{"content":"B. Steichen, G. Carenini, and C. Conati. 2013. User-adaptive information visualization: using eye gaze data to infer visualization tasks and user cognitive abilities. In Proc. IUI. 317--328. http://dx.doi.org/10.1145/2449396.2449439","paperID":"None"},{"content":"S. Stellmach and R. Dachselt. 2012. Look & touch: gaze-supported target acquisition. In Proc. CHI. 2981--2990. http://dx.doi.org/10.1145/2207676.2208709","paperID":"None"},{"content":"J. D. Still and C. M. Masciocchi. 2010. A Saliency Model Predicts Fixations in Web Interfaces. In Proc. MDDAUI.","paperID":"None"},{"content":"Y. Sugano, Y. Matsushita, Y. Sato, and H. Koike. 2015. Appearance-Based Gaze Estimation With Online Calibration From Mouse Operations. IEEE THMS PP, 99 (2015), 1--11. http://dx.doi.org/10.1109/THMS.2015.2400434","paperID":"None"},{"content":"D. Toker, C. Conati, B. Steichen, and G. Carenini. 2013. Individual user characteristics and information visualization: connecting the dots through eye tracking. In Proc. CHI. 295--304. http://dx.doi.org/10.1145/2470654.2470696","paperID":"None"},{"content":"J. Turner, A. Bulling, J. Alexander, and H. Gellersen. 2014. Cross-Device Gaze-Supported Point-to-Point Content Transfer. In Proc. ETRA. 19--26. http://dx.doi.org/10.1145/2578153.2578155","paperID":"None"},{"content":"M. Vidal, A. Bulling, and H. Gellersen. 2013. Pursuits: Spontaneous Interaction with Displays based on Smooth Pursuit Eye Movement and Moving Targets. In Proc. UbiComp. 439--448. http://dx.doi.org/10.1145/2468356.2479632","paperID":"None"},{"content":"Å. Wengelin, M. Torrance, K. Holmqvist, S. Simpson, D. Galbraith, V. Johansson, and R. Johansson. 2009. Combined eyetracking and keystroke-logging methods for studying cognitive processes in text production. Behavior research methods 41, 2 (2009), 337--351. http://dx.doi.org/10.3758/BRM.41.2.337","paperID":"None"},{"content":"B. Westphal and T. Syeda-Mahmood. 2002. On learning video browsing behavior from user interactions. In Proc. WWW.","paperID":"None"},{"content":"E. Wood and A. Bulling. 2014. EyeTab: Model-based gaze estimation on unmodified tablet computers. In Proc. ETRA. 207--210. http://dx.doi.org/10.1145/2578153.2578185","paperID":"None"},{"content":"L. Zhang, M. H. Tong, T. K. Marks, H. Shan, and G. W. Cottrel. 2008. SUN: A Bayesian framework for saliency using natural statistics. Journal of Vision (2008). http://dx.doi.org/10.1167/8.7.32","paperID":"None"},{"content":"X. Zhang, Y. Sugano, M. Fritz, and A. Bulling. 2015. Appearance-Based Gaze Estimation in the Wild. In Proc. CVPR. 4511--4520. http://dx.doi.org/10.1109/CVPR.2015.7299081","paperID":"None"}],"title":"Spatio-Temporal Modeling and Prediction of Visual Attention in Graphical User Interfaces","filename":"CHI16/p3299","authors":["Pingmei Xu","Yusuke Sugano","Andreas Bulling"],"conference":"CHI '16"}