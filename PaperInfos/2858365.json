{"paperId":2858365,"citation":[],"abstract":"Tools that provide visual feedback about emotions to the user in the form of an avatar or an emoticon have become increasingly important. While a great deal of effort has already been put into the reliable and accurate automatic detection of emotions, only very little is known about how this information about affective states should be displayed in a comprehensible way to the user. In the present study, three newly developed feedback tools were evaluated. The tools were developed on the basis of an existing non-verbal questionnaire to represent two dimensions of emotion (i.e. valence and arousal) based on the circumplex model of affect. A total number of 826 participants were tested, using different vignettes that describe situations with specific affective content. Employing three newly developed affective feedback tools (AniSAM, AniAvatar and MergedSAM), the ratings obtained were compared to ratings using the original SAM instrument, a well-established questionnaire to measure affect. Results indicated that the animated feedback increased the accuracy of the arousal representation. Furthermore, valence feedback was more accurate when provided with an animated manikin-based tool rather than an avatar-based tool. This provided first evidence for the usefulness of animated tools offering visual feedback on user emotion. All instruments need to undergo further development. AniSAM and AniAvatar can be downloaded for purposes of practical applications and further research.","reference":[{"content":"Bower, G. H. (1981). Mood and memory. American Psychologist, 36(2), 129-148.","paperID":"None"},{"content":"Bradley, M. M., & Lang, P. J. (1994). Measuring emotion: the self-assessment manikin and the semantic differential. Journal of Behavior Therapy and Experimental Psychiatry, 25(1), 49-59.","paperID":"None"},{"content":"Byrne, M. D. (1993). Using icons to find documents: Simplicity is critical. Proceedings of INTERCHI '93, 446 - 453.","paperID":"None"},{"content":"Chovil, N. (1991). Discourse-oriented facial displays in conversation. Research on Language & Social Interaction, 25(1--4), 163-194.","paperID":"None"},{"content":"Colquitt, J. A., LePine, J. A., & Noe, R. A. (2000). Toward an integrative theory of training motivation: A meta-analytic path analysis of 20 years of research. Journal of Applied Psychology, 85(5), 678-707.","paperID":"None"},{"content":"Damasio, A. (1994). Descartes' error. NY: Avon.","paperID":"None"},{"content":"De la Torre, F., & Cohn, J. F. (2011). Visual Analysis of Humans. In T. B. Moeslund, A. Hilton, V. Krüger, & L. Sigal (Eds.), Visual Analysis of Humans (pp. 377-409). London: Springer.","paperID":"None"},{"content":"Derryberry, D., & Tucker, D. M. (1992). Neural mechanisms of emotion. Journal of Consulting and Clinical Psychology, 60(3), 329-338.","paperID":"None"},{"content":"Desmet, P. M. A. (2005). Measuring Emotions. In M. A. Blythe, A. F. Monk, K. Overbeeke, & P. C. Wright (Eds.), Funology: from usability to enjoyment (pp. 111- 124). Dordrecht: Kluwer Academic Publishers.","paperID":"None"},{"content":"El Kaliouby, R., & Robinson, P. (2004). FAIM: integrating automated facial affect analysis in instant messaging. In Proceedings of the 9th international conference on Intelligent user interface IUI '04 (pp. 244-246). NY, NY, USA: ACM Press.","paperID":"None"},{"content":"Fasel, B., & Luettin, J. (2003). Automatic facial expression analysis: a survey. Pattern Recognition, 36(1), 259-275.","paperID":"None"},{"content":"Forsythe, A., Mulhern, G., & Sawey, M. (2008). Confounds in pictorial sets: The role of complexity and familiarity in basic-level picture processing. Behavior Research Methods, 40(1), 116-129.","paperID":"None"},{"content":"Frijda N (1986). The emotions. NY: Cambridge University Press.","paperID":"None"},{"content":"Frijda, N. (2005). Emotion experience. Cognition & Emotion, 19(4), 473-497.","paperID":"None"},{"content":"Gardhouse, K., & Anderson, A. K. (2013). Objective and subjective measurements in affective science. In J. Armony & P. Vuilleumier (Eds.), The Cambridge handbook of human affective neuroscience (pp. 57-81). Cambridge: Cambridge University Press.","paperID":"None"},{"content":"Gerber, A. J., Posner, J., Gorman, D., Colibazzi, T., Yu, S., Wang, Z., ... Peterson, B. S. (2008). An Affective Circumplex Model of Neural Systems Subserving Valence, Arousal, & Cognitive Overlay During the Appraisal of Emotional Faces. Neuropsychologia, 46(8), 2129-2139.","paperID":"None"},{"content":"Gobron, S., Ahn, J., Paltoglou, G., Thelwall, M., & Thalmann, D. (2010). From sentence to emotion: a realtime three-dimensional graphics metaphor of emotions extracted from text. The Visual Computer, 26(6--8), 505- 519.","paperID":"None"},{"content":"Isherwood, S. J., McDougall, S. J., & Curry, M. B. (2007). Icon identification in context: The changing role of icon characteristics with user experience. Human Factors: The Journal of the Human Factors and Ergonomics Society, 49(3), 465-476.","paperID":"None"},{"content":"Kapoor, A., & Picard, R. W. (2005). Multimodal affect recognition in learning environments. In Proceedings of the 13th annual ACM international conference on Multimedia - MULTIMEDIA '05 (pp. 677-682). NY, NY, USA: ACM Press.","paperID":"None"},{"content":"Ku, J., Jang, H. J., Kim, K. U., Kim, J. H., Park, S. H., Lee, J. H., Kim,J.J.; Kim, I.Y., & Kim, S. I. (2005). Experimental results of affective valence and arousal to avatar's facial expressions. Cyberpsychology & Behavior, 8(5), 493-503.","paperID":"None"},{"content":"Larsen, R. J.. & Diener, E. (1992). Promises and problems with the circumplex model of emotion. In M. S. Clark (Ed.). Review of personality and social psychology: Emotion (Vol. 13. pp. 25--59). Newbury Park. CA: Sage","paperID":"None"},{"content":"Lisetti, C. L., & Nasoz, F. (2002). MAUI: a Multimodal Affective User Interface. In Proceedings of the tenth ACM international conference on Multimedia MULTIMEDIA '02 (pp. 161-170). NY: ACM Press.","paperID":"None"},{"content":"Mayer JD, Allen JP, Beauregard K. (1995). Mood inductions for four specific moods: A procedure employing guided imagery vignettes with music. Journal of Mental Imagery, 19,151-159.","paperID":"None"},{"content":"McDougall, S. J. P., de Bruijn, O., & Curry, M. B. (2000). Exploring the effects of icon characteristics on user performance: The role of icon concreteness, complexity, and distinctiveness. Journal of Experimental Psychology: Applied, 6, 291-306.","paperID":"None"},{"content":"McDuff, D., Karlson, A., Kapoor, A., Roseway, A., & Czerwinski, M. (2012). AffectAura: an intelligent system for emotional memory. In CHI '12 Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (pp. 849-858). NY, NY, USA: ACM .","paperID":"None"},{"content":"Mehrabian, A., & Russell, J. A. (1974). An approach to environmental psychology. Cambridge: The MIT Press.","paperID":"None"},{"content":"Morishima, S. (2003). Modelling of facial expression and emotion for human communication system. Displays (17), 15-25.","paperID":"None"},{"content":"Nasoz, F., Alvarez, K., Lisetti, C. L., & Finkelstein, N. (2004). Emotion recognition from physiological signals using wireless sensors for presence technologies. Cognition, Technology & Work, 6(1), 4-14.","paperID":"None"},{"content":"Nicolaou, M. A., Gunes, H., & Pantic, M. (2011). Continuous Prediction of Spontaneous Affect from Multiple Cues and Modalities in Valence-Arousal Space. IEEE Transactions on Affective Computing, 2(2), 92-105.","paperID":"None"},{"content":"Peter, C., & Herbon, A. (2006). Emotion representation and physiology assignments in digital systems. Interacting with Computers, 18, 139-170.","paperID":"None"},{"content":"Pickles, J. (1995). Ground Truth: The Social Implications of Geographical Information Systems. NY: Guilford Press.","paperID":"None"},{"content":"Russell, J. A. (1980). A circumplex model of affect. Journal of Personality and Social Psychology, 39(6), 1161-1178.","paperID":"None"},{"content":"Ringeval, F., Sonderegger, A., Noris, B., Billard, A., Sauer, J., & Lalanne, D. (2013). On the Influence of Emotional Feedback on Emotion Recognition and Gaze Behavior. In proceedings of the 5th biannual Humaine Association Conference on Affective Computing and Intelligent Interaction (ACII 2013), IEEE.","paperID":"None"},{"content":"Rizzo, A. a., Neumann, U., Enciso, R., Fidaleo, D., & Noh, J. y. (2001). Performance-Driven Facial Animation: Basic Research on Human Judgments of Emotional State in Facial Avatars. Cyber-Psychology & Behavior, 4(4), 471-487.","paperID":"None"},{"content":"Robinson, M. D., Storbeck, J., Meier, B. P., & Kirkeby, B. S. (2004). Watch out! That could be dangerous: valence-arousal interactions in evaluative processing. Personality & Social Psychology Bulletin, 30(11), 1472-1484.","paperID":"None"},{"content":"Sander, D. (2013). Models of emotions - the affective neuroscience approach. In J. Armony & P. Vuilleumier (Eds.), The Cambridge handbook of human affective neuroscience (pp. 5-53). Cambridge: Cambridge University Press.","paperID":"None"},{"content":"Scherer, K. R. (2005). What are emotions? And how can they be measured? Social Science Information, 44(4), 695-729.","paperID":"None"},{"content":"Schuller, B., Vlasenko, B., Eyben, F., Rigoll, G., & Wendemuth, A. (2009). Acoustic emotion recognition: A benchmark comparison of performances. 2009 IEEE Workshop on Automatic Speech Recognition & Understanding, 552-557.","paperID":"None"},{"content":"Shih, T.-H., & Fan, X. (2008). Comparing Response Rates from Web and Mail Surveys: A Meta-Analysis. Field Methods, 20(3), 249-271.","paperID":"None"},{"content":"Sonderegger, A., Lalanne, D., Bergholz, L., Ringeval, F., & Sauer, J. (2013). Computer-supported work in distributed and non-distributed teams: the influence of mood feedback. In P. Kotz et al. (Eds.): INTERACT 2013, Part II, LNCS 8118, pp. 445--460. IFIP International Federation for Information Processing.","paperID":"None"},{"content":"Spencer-Smith, J., Wild, H., Innes-Ker, A.H., et al. (2001). Making faces: creating three-dimensional parameterized models of facial expression. Behavioral Research Methods Instrumental Computations (33), 115-123.","paperID":"None"},{"content":"Ståhl, A., Höök, K., Svensson, M., Taylor, A. S., & Combetto, M. (2008). Experiencing the Affective Diary. Personal and Ubiquitous Computing, 13(5), 365-378.","paperID":"None"},{"content":"Tan, C. S. S., Schöning, J., Luyten, K., & Coninx, K. (2013). Informing intelligent user interfaces by inferring affective states from body postures in ubiquitous computing environments. Proceedings of the 2013 International Conference on Intelligent User Interfaces IUI '13, pp. 235--264.","paperID":"None"},{"content":"Vogt, T., André, E., & Bee, N. (2008). EmoVoice -- A Framework for Online Recognition of Emotions from Voice. In E. André, L. Dybkjær, W. Minker, H. Neumann, R. Pieraccini, & M. Weber (Eds.), Perception in Multimodal Dialogue Systems (p. 188-199). Berlin, Heidelberg: Springer.","paperID":"None"},{"content":"Wallbott, H., & Scherer, K. (1989). Assessing emotion by questionnaire. In R. Plutchik & H. Kellerman (Eds.), Emotion: Theory, research, and experience (pp. 55-82). San Diego: Academic Press.","paperID":"None"},{"content":"Wallraven, C., Breidt, M., Cunningham, D. W., & Bülthoff, H. H. (2005). Psychophysical Evaluation of Animated Facial Expressions. In Proceedings of the 2Nd Symposium on Applied Perception in Graphics and Visualization (pp. 17-24). NY, NY, USA: ACM.","paperID":"None"},{"content":"Zajonc, R. (1984). On the primacy of affect. American Psychologist, 39(2), 117-12","paperID":"None"}],"title":"AniSAM & AniAvatar: Animated Visualizations of Affective States","filename":"CHI16/p4828","authors":["Andreas Sonderegger","Klaus Heyden","Alain Chavaillaz","Juergen Sauer"],"conference":"CHI '16"}