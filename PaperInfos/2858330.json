{"paperId":2858330,"abstract":"Crowd feedback services offer a new method for acquiring feedback during design. A key problem is that the services only return the feedback without any cues about the people who provided it. In this paper, we investigate two cues of a feedback provider -- the effort invested in a feedback task and expertise in the domain. First, we tested how positive and negative cues of a provider's effort and expertise affected perceived quality of the feedback. Results showed both cues affected perceived quality, but primarily when the cues were negative. The results also showed that effort cues affected perceived quality as much as expertise. In a second study, we explored the use of behavioral data for modeling effort for feedback tasks. For a binary classification, the models achieved up to 92% accuracy relative to human raters. This result validates the feasibility of implementing effort cues in crowd services. The contributions of this work will enable increased transparency in crowd feedback services, benefiting both designers and feedback providers.","reference":[{"content":"Vamshi Ambati, Stephan Vogel and Jaime Carbonell. Towards Task Recommendation in Micro-Task Markets. Proceedings of the AAAI Workshop on Human Computation (HCOMP), 2011.","paperID":"None"},{"content":"Michael H. Birnbaum and Steven E. Stegner. Source credibility in social judgment: Bias, expertise, and the judge's point of view. Journal of Personality and Social Psychology, 37 (1): 48--74. http://dx.doi.org/10.1037/0022--3514.37.1.48","paperID":"None"},{"content":"Caleb T. Carr and Joseph B. Walther. Increasing attributional certainty via social media: Learning about others one bit at a time. Journal of Computer-Mediated Communication, 19 (4): 922--937. http://dx.doi.org/10.1111/jcc4.12072","paperID":"None"},{"content":"Justin Cheng, Jaime Teevan and Michael S. Bernstein. Measuring Crowdsourcing Effort with Error-Time Curves. Proceedings of the ACM Conference on Human Factors in Computing Systems, 2015, 13651374. http://doi.acm.org/10.1145/2702123.2702145","paperID":"None"},{"content":"Steven Dow, Julie Fortuna, Dan Schwartz, Beth Altringer, Daniel Schwartz and Scott Klemmer. Prototyping dynamics: sharing multiple designs improves exploration, group rapport, and results. Proceedings of the ACM Conference on Human Factors in Computing Systems, 2011, 2807--2816. http://doi.acm.org/10.1145/1978942.1979359","paperID":"None"},{"content":"James Fogarty, Scott E. Hudson, Christopher G. Atkeson, Daniel Avrahami, Jodi Forlizzi, Sara Kiesler, Johnny C. Lee and Jie Yang. Predicting human interruptibility with sensors. ACM Transactions on Computer-Human Interaction, 12 (1): 119--146. http://doi.acm.org/10.1145/1057237.1057243","paperID":"None"},{"content":"B. J. Fogg, Jonathan Marshall, Othman Laraki, Alex Osipovich, Chris Varma, Nicholas Fang, Jyoti Paul, Akshay Rangnekar, John Shon, Preeti Swani and Marissa Treinen. What makes Web sites credible?: A report on a large quantitative study. Proceedings of the ACM Conference on Human Factors in Computing Systems, 2001, 61--68. http://doi.acm.org/10.1145/365024.365037","paperID":"None"},{"content":"Krzysztof Z. Gajos, Katharina Reinecke and Charles Herrmann. Accurate Measurements of Pointing Performance from In Situ Observations. Proceedings of the ACM Conference on Human Factors in Computing Systems, 2012, 3157--3166. http://doi.acm.org/10.1145/2207676.2208733","paperID":"None"},{"content":"Yihan Gao and Aditya Parameswaran. Finish them!: pricing algorithms for human computation. Proceedings VLDB Endowment, 7 (14): 1965--1976. http://dx.doi.org/10.14778/2733085.2733101","paperID":"None"},{"content":"Arin Ghazarian and S. Majid Noorhosseini. Automatic detection of users' skill levels using high-frequency user interface events. User Modeling and User-Adapted Interaction, 20 (2): 109--146. http://dx.doi.org/10.1007/s11257-010--9073--5","paperID":"None"},{"content":"Michael D. Greenberg, Matthew W. Easterday and Elizabeth M. Gerber. Critiki : A Scaffolded Approach to Gathering Design Feedback from Paid Crowdworkers. Proceedings of the ACM Conference on Creativity & Cognition, 2015, 235--244. http://doi.acm.org/10.1145/2757226.2757249","paperID":"None"},{"content":"Shih-wen Huang and Wai-tat Fu. Don't Hide in the Crowd! Increasing Social Transparency Between Peer Workers Improves Crowdsourcing Outcomes. Proceedings of the ACM Conference on Human Factors in Computing Systems, 2013, 621--630. http://doi.acm.org/10.1145/2470654.2470743","paperID":"None"},{"content":"Amy Hurst, Scott E. Hudson and Jennifer Mankoff. Dynamic detection of novice vs. skilled use without a task model. Proceedings of the ACM Conference on Human Factors in Computing Systems, 2007, 271--280. http://doi.acm.org/10.1145/1240624.1240669","paperID":"None"},{"content":"David E. Kanouse and L. Hanson. Negativity in evaluations. in Attribution: Perceiving the Causes of Behavior, General Learning Press, Morristown, NJ, 1972, 47--62.","paperID":"None"},{"content":"David E. Kanouse. Explaining negativity biases in evaluation and choice behavior: Theory and research. Advances in Consumer Research, 11 (1): 703--708.","paperID":"None"},{"content":"Aniket Kittur, Ed H. Chi and Bongwon Suh. Crowdsourcing user studies with Mechanical Turk. Proceedings of the ACM Conference on Human Factors in Computing Systems, 2008, 453--456. http://doi.acm.org/10.1145/1357054.1357127","paperID":"None"},{"content":"Aniket Kittur, Boris Smus, Susheel Khamkar and Robert E. Kraut. CrowdForge: crowdsourcing complex work. Proceedings of the ACM Symposium on User Interface Software and Technology, 2011, 43--52. http://doi.acm.org/10.1145/2047196.2047202","paperID":"None"},{"content":"Karl Christoph Klauer. Affective Priming. European Review of Social Psychology, 8 (1): 67--103. http://dx.doi.org/10.1080/14792779643000083","paperID":"None"},{"content":"Q. Vera Liao and Wai-tat Fu. Expert Voices in Echo Chambers: Effects of Source Expertise Indicators on Exposure to Diverse Opinions. Proceedings of the ACM Conference on Human Factors in Computing Systems, 2014, 2745--2754. http://dx.doi.org/10.1145/2556288.2557240","paperID":"None"},{"content":"Kurt Luther, Kelly Caine, Kevin Ziegler and Amy Bruckman. Why It Works (When It Works): Success Factors in Online Creative Collaboration. Proceedings of the ACM Conference on Supporting Group Work, 2010, 1--10. http://doi.acm.org/10.1145/1880071.1880073","paperID":"None"},{"content":"Kurt Luther, Jari-lee Tolentino, Wei Wu, Amy Pavel, Brian P. Bailey, Maneesh Agrawala and Steven P. Dow. Structuring, Aggregating, and Evaluating Crowdsourced Design Critique. Proceedings of the ACM Conference on Computer Supported Cooperative Work, 2015, 473--485. http://dx.doi.org/10.1145/2675133.2675283","paperID":"None"},{"content":"Jennifer Marlow and Laura Dabbish. The Effects of Visualizing Activity History on Attitudes and Behaviors in a Peer Production Context. Proceedings of the ACM Conference on Computer Supported Cooperative Work, 2015, 757--764. http://dx.doi.org/10.1145/2675133.2675250","paperID":"None"},{"content":"Jennifer Marlow and Laura Dabbish. From rookie to all-star: professional development in a graphic design social networking site. Proceedings of the ACM Conference on Computer Supported Cooperative Work, 2014, 922--933. http://doi.acm.org/10.1145/2531602.2531651","paperID":"None"},{"content":"Jennifer Marlow, Laura Dabbish and Jodi Forlizzi. Exploring the Role of Activity Trace Design on Evaluations of Online Worker Quality. Proceedings of the ACM Conference on Human Factors in Computing Systems, 2015, 1617--1620. http://dx.doi.org/10.1145/2702123.2702195","paperID":"None"},{"content":"Jennifer Marlow, Laura Dabbish and Jim Herbsleb. Impression Formation in Online Peer Production: Activity Traces and Personal Profiles in GitHub. Proceedings of the ACM Conference on Computer Supported Cooperative Work, 2013, 117--128. http://dx.doi.org/10.1145/2441776.2441792","paperID":"None"},{"content":"Elliott McGinnies and Charles D. Ward. Better Liked than Right: Trustworthiness and Expertise as Factors in Credibility. Personality and Social Psychology Bulletin, 6 (3): 467--472. http://dx.doi.org/10.1177/014616728063023","paperID":"None"},{"content":"Miriam J. Metzger, Andrew J. Flanagin and Ryan B. Medders. Social and heuristic approaches to credibility evaluation online. Journal of Communication, 60, 413439. http://dx.doi.org/10.1111/j.1460- 2466.2010.01488.x","paperID":"None"},{"content":"Tanushree Mitra, C. J. Hutto and E. Gilbert. Comparing person- and process-centric strategies for obtaining quality data on Amazon Mechanical Turk. Proceedings of the ACM Conference on Human Factors in Computing Systems, 2015, 1345--1354. http://dx.doi.org/10.1145/2702123.2702553","paperID":"None"},{"content":"G. Peeters and J. Czapinski. Positive-negative asymmetry in evaluations: The distinction between affective and informational negativity effects. European Review of Social Psychology, 1, 33--60. http://dx.doi.org/10.1080/14792779108401856","paperID":"None"},{"content":"Drazen Prelec. A Bayesian truth serum for subjective data. Science, 306, 462--466. http://dx.doi.org/10.1126/science.1102081","paperID":"None"},{"content":"Jeffrey M. Rzeszotarski and Aniket Kittur. CrowdScape: Interactively Visualizing User Behavior and Output. Proceedings of the ACM Symposium on User Interface Software and Technology, 2012, 55--62. http://dx.doi.org/10.1145/2380116.2380125","paperID":"None"},{"content":"Jeffrey M. Rzeszotarski and Aniket Kittur. Instrumenting the Crowd: Using Implicit Behavioral Measures to Predict Task Performance. Proceedings of the ACM Symposium on User Interface Software and Technology, 2011, 13--22. http://doi.acm.org/10.1145/2047196.2047199","paperID":"None"},{"content":"James Shanteau, David J. Weiss, Rickey P. Thomas and Julia C. Pounds. Performance-based assessment of expertise: How to decide if someone is an expert or not. European Journal of Operational Research, 136 (2): 253--263. http://dx.doi.org/10.1016/S0377- 2217(01)00113--8","paperID":"None"},{"content":"Pao Siangliulue, Kenneth C. Arnold, Krzysztof Z. Gajos and Steven P. Dow. Toward Collaborative Ideation at Scale: Leveraging Ideas from Others to Generate More Creative and Diverse Ideas. Proceedings of the ACM Conference on Computer Supported Cooperative Work, 2015, 937--945. http://doi.acm.org/10.1145/2675133.2675239","paperID":"None"},{"content":"H. Colleen Stuart, Laura Dabbish, Sara Kiesler, Peter Kinnaird and Ruogu Kang. Social Transparency in Networked Information Exchange: A Framework and Research Question. Proceedings of the ACM Conference on Computer Supported Cooperative Work, 2012, 451--460. http://doi.acm.org/10.1145/2145204.2145275","paperID":"None"},{"content":"Wesley Willett, Jeffrey Heer and Maneesh Agrawala. Strategies for crowdsourcing social data analysis. Proceedings of the ACM Conference on Human Factors in Computing Systems, 2012, 227--236. http://doi.acm.org/10.1145/2207676.2207709","paperID":"None"},{"content":"Anbang Xu, Shih-wen Huang and Brian P. Bailey. Voyant : Generating Structured Feedback on Visual Designs Using a Crowd of Non-Experts. Proceedings of the ACM Conference on Computer Supported Cooperative Work, 2014, 1433--1444. http://dx.doi.org/10.1145/2531602.2531604","paperID":"None"},{"content":"Anbang Xu, Huaming Rao, Steven P. Dow and Brian P. Bailey. A Classroom Study of Using Crowd Feedback in the Iterative Design Process. Proceedings of the ACM Conference on Computer Supported Cooperative Work, 2015, 1637--1648. http://dx.doi.org/10.1145/2675133.2675140","paperID":"None"}],"citation":[],"title":"Novices Who Focused or Experts Who Didn't?","filename":"CHI16/p4086","authors":["Y. Wayne Wu","Brian P. Bailey"],"conference":"CHI '16"}