{"paperId":2470684,"reference":[{"content":"Chris Callison-Burch, Fast, cheap, and creative: evaluating translation quality using Amazon's Mechanical Turk, Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1, August 06-07, 2009, Singapore","paperID":"1699548"},{"content":"Chandler, D., and Kapelner, A. Breaking monotony with meaning: Motivation in crowdsourcing markets. Working Paper, May 2010.","paperID":"None"},{"content":"Forrester Cole , Kevin Sanik , Doug DeCarlo , Adam Finkelstein , Thomas Funkhouser , Szymon Rusinkiewicz , Manish Singh, How well do line drawings depict shape?, ACM SIGGRAPH 2009 papers, August 03-07, 2009, New Orleans, Louisiana","paperID":"1531334"},{"content":"Devore, J. Probability and Statistics for Engineering and the Sciences, seventh ed. Thomson Higher Education, 2008.","paperID":"None"},{"content":"Krzysztof Z. Gajos , Mary Czerwinski , Desney S. Tan , Daniel S. Weld, Exploring the design space for adaptive graphical user interfaces, Proceedings of the working conference on Advanced visual interfaces, May 23-26, 2006, Venezia, Italy","paperID":"1133306"},{"content":"Krzysztof Z. Gajos , Katherine Everitt , Desney S. Tan , Mary Czerwinski , Daniel S. Weld, Predictability and accuracy in adaptive user interfaces, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, April 05-10, 2008, Florence, Italy","paperID":"1357252"},{"content":"Tovi Grossman , Ravin Balakrishnan, The bubble cursor: enhancing target acquisition by dynamic resizing of the cursor's activation area, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, April 02-07, 2005, Portland, Oregon, USA","paperID":"1055012"},{"content":"Jeffrey Heer , Michael Bostock, Crowdsourcing graphical perception: using mechanical turk to assess visualization design, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, April 10-15, 2010, Atlanta, Georgia, USA","paperID":"1753357"},{"content":"Horton, J. J., Rand, D. G., and Zeckhauser, R. J. The online laboratory: Conducting experiments in a real labor market. Experimental Economics (2011).","paperID":"None"},{"content":"Kapelner, A., and Chandler, D. Preventing Satisficing in Online Surveys: A \"Kapcha\" to Ensure Higher Quality Data. In CrowdConf (2010).","paperID":"None"},{"content":"Aniket Kittur , Ed H. Chi , Bongwon Suh, Crowdsourcing user studies with Mechanical Turk, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, April 05-10, 2008, Florence, Italy","paperID":"1357127"},{"content":"Nicholas Kong , Jeffrey Heer , Maneesh Agrawala, Perceptual Guidelines for Creating Rectangular Treemaps, IEEE Transactions on Visualization and Computer Graphics, v.16 n.6, p.990-998, November 2010","paperID":"1907982"},{"content":"Greg Little , Lydia B. Chilton , Max Goldman , Robert C. Miller, TurKit: human computation algorithms on mechanical turk, Proceedings of the 23nd annual ACM symposium on User interface software and technology, October 03-06, 2010, New York, New York, USA","paperID":"1866040"},{"content":"Mao, A., Chen, Y., Gajos, K., Parkes, D., Procaccia, A., and Zhang, H. Turkserver: Enabling synchronous and longitudinal online experiments. In Proceedings of HCOMP'12 (2012).","paperID":"None"},{"content":"Mason, W., and Suri, S. Conducting behavioral research on amazon's mechanical turk. Behavior Research Methods (2010), 1--23.","paperID":"None"},{"content":"Winter Mason , Duncan J. Watts, Financial incentives and the \"performance of crowds\", Proceedings of the ACM SIGKDD Workshop on Human Computation, June 28-28, 2009, Paris, France","paperID":"1600175"},{"content":"Jon Noronha , Eric Hysen , Haoqi Zhang , Krzysztof Z. Gajos, Platemate: crowdsourcing nutritional analysis from food photographs, Proceedings of the 24th annual ACM symposium on User interface software and technology, October 16-19, 2011, Santa Barbara, California, USA","paperID":"2047198"},{"content":"Oleson, D., Sorokin, A., Laughlin, G., Hester, V., Le, J., and Biewald, L. Programmatic gold: Targeted and scalable quality assurance in crowdsourcing. In Proceedings of HCOMP'11 (2011).","paperID":"None"},{"content":"Oppenheimer, D., Meyvis, T., and Davidenko, N. Instructional manipulation checks: Detecting satisficing to increase statistical power. Journal of Experimental Social Psychology 45, 4 (2009), 867--872.","paperID":"None"},{"content":"Paolacci, G., Chandler, J., and Ipeirotis, P. Running experiments on amazon mechanical turk. Judgment and Decision Making 5, 5 (2010), 411--419.","paperID":"None"},{"content":"Prelec, D. A Bayesian Truth Serum for Subjective Data. Science 306, 5695 (Oct. 2004), 462--466.","paperID":"None"},{"content":"Rand, D. G. The promise of Mechanical Turk: How online labor markets can help theorists run behavioral experiments. Journal of theoretical biology (2012).","paperID":"None"},{"content":"Jeffrey M. Rzeszotarski , Aniket Kittur, Instrumenting the crowd: using implicit behavioral measures to predict task performance, Proceedings of the 24th annual ACM symposium on User interface software and technology, October 16-19, 2011, Santa Barbara, California, USA","paperID":"2047199"},{"content":"Schmidt, L. Crowdsourcing for human subjects research. In CrowdConf (2010).","paperID":"None"},{"content":"Andrew Sears , Ben Shneiderman, Split menus: effectively using selection frequency to organize menus, ACM Transactions on Computer-Human Interaction (TOCHI), v.1 n.1, p.27-51, March 1994","paperID":"174632"},{"content":"Aaron D. Shaw , John J. Horton , Daniel L. Chen, Designing incentives for inexpert human raters, Proceedings of the ACM 2011 conference on Computer supported cooperative work, March 19-23, 2011, Hangzhou, China","paperID":"1958865"},{"content":"Suri, S., and Watts, D. Cooperation and contagion in networked public goods experiments. Arxiv preprint arXiv10081276 (2010).","paperID":"None"}],"citation":[{"content":"Sandy J.J. Gould , Anna L. Cox , Duncan P. Brumby, Task Lockouts Induce Crowdworkers to Switch to Other Activities, Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems, April 18-23, 2015, Seoul, Republic of Korea","paperID":"2732709"},{"content":"Abhishek Chakraborty, Scranvas: Gamified Forum for Amateur Designersto Share Creative Work and Generate Constructive Feedback, Proceedings of the 7th International Conference on HCI, IndiaHCI 2015, p.145-148, December 17-19, 2015, Guwahati, India","paperID":"2836286"},{"content":"Abhishek Chakraborty, Scranvas: Gamified Forum for Amateur Designers to Share Creative Work and Generate Constructive Feedback, Proceedings of the 2015 ACM SIGCHI Conference on Creativity and Cognition, June 22-25, 2015, Glasgow, United Kingdom","paperID":"2764557"},{"content":"Ailbhe Finnerty , Pavel Kucherbaev , Stefano Tranquillini , Gregorio Convertino, Keep it simple: reward and task design in crowdsourcing, Proceedings of the Biannual Conference of the Italian Chapter of SIGCHI, p.1-4, September 16-20, 2013, Trento, Italy","paperID":"2499168"},{"content":"Michael Nebeling , Maximilian Speicher , Moira C. Norrie, CrowdStudy: general toolkit for crowdsourced evaluation of web interfaces, Proceedings of the 5th ACM SIGCHI symposium on Engineering interactive computing systems, June 24-27, 2013, London, United Kingdom","paperID":"2480303"},{"content":"Harini Alagarai Sampath , Rajeev Rajeshuni , Bipin Indurkhya, Cognitively inspired task design to improve user performance on crowdsourcing platforms, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, April 26-May 01, 2014, Toronto, Ontario, Canada","paperID":"2557155"},{"content":"Louis Li , Krzysztof Z. Gajos, Adaptive click-and-cross: adapting to both abilities and task improves performance of users with impaired dexterity, Proceedings of the 19th international conference on Intelligent User Interfaces, February 24-27, 2014, Haifa, Israel","paperID":"2557511"},{"content":"Joohee Choi , Heejin Choi , Woonsub So , Jaeki Lee , Jongjun You, A Study about Designing Reward for Gamified Crowdsourcing System, Proceedings of the Third International Conference on Design, User Experience, and Usability. User Experience Design for Diverse Interaction Platforms and Environments, June 22-27, 2014","paperID":"2694938"},{"content":"Simon Breslav , Azam Khan , Kasper Hornbæk, Mimic: visual analytics of online micro-interactions, Proceedings of the 2014 International Working Conference on Advanced Visual Interfaces, May 27-29, 2014, Como, Italy","paperID":"2598168"},{"content":"Nada Sherief , Nan Jiang , Mahmood Hosseini , Keith Phalp , Raian Ali, Crowdsourcing software evaluation, Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering, May 13-14, 2014, London, England, United Kingdom","paperID":"2601300"},{"content":"Anbang Xu , Huaming Rao , Steven P. Dow , Brian P. Bailey, A Classroom Study of Using Crowd Feedback in the Iterative Design Process, Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing, March 14-18, 2015, Vancouver, BC, Canada","paperID":"2675140"},{"content":"Anbang Xu , Shih-Wen Huang , Brian Bailey, Voyant: generating structured feedback on visual designs using a crowd of non-experts, Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing, February 15-19, 2014, Baltimore, Maryland, USA","paperID":"2531604"},{"content":"Katharina Reinecke , Krzysztof Z. Gajos, LabintheWild: Conducting Large-Scale Online Experiments With Uncompensated Samples, Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing, March 14-18, 2015, Vancouver, BC, Canada","paperID":"2675246"},{"content":"Esben W. Pedersen , Sriram Subramanian , Kasper Hornbæk, Is my phone alive?: a large-scale study of shape change in handheld devices using videos, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, April 26-May 01, 2014, Toronto, Ontario, Canada","paperID":"2557018"},{"content":"Paul van Schaik , Matthew Weston, Magnitude-based inference and its application in user research, International Journal of Human-Computer Studies, v.88 n.C, p.38-50, April 2016","paperID":"2895151"}],"abstract":"Online labor markets, such as Amazon's Mechanical Turk (MTurk), provide an attractive platform for conducting human subjects experiments because the relative ease of recruitment, low cost, and a diverse pool of potential participants enable larger-scale experimentation and faster experimental revision cycle compared to lab-based settings. However, because the experimenter gives up the direct control over the participants' environments and behavior, concerns about the quality of the data collected in online settings are pervasive. In this paper, we investigate the feasibility of conducting online performance evaluations of user interfaces with anonymous, unsupervised, paid participants recruited via MTurk. We implemented three performance experiments to re-evaluate three previously well-studied user interface designs. We conducted each experiment both in lab and online with participants recruited via MTurk. The analysis of our results did not yield any evidence of significant or substantial differences in the data collected in the two settings: All statistically significant differences detected in lab were also present on MTurk and the effect sizes were similar. In addition, there were no significant differences between the two settings in the raw task completion times, error rates, consistency, or the rates of utilization of the novel interaction mechanisms introduced in the experiments. These results suggest that MTurk may be a productive setting for conducting performance evaluations of user interfaces providing a complementary approach to existing methodologies.","video":"http://www.youtube.com/embed/VPZbYyEO7Hk?rel=0","title":"Crowdsourcing performance evaluations of user interfaces","filename":"CHI13/p207","authors":["Steven Komarov","Katharina Reinecke","Krzysztof Z. Gajos"],"conference":"CHI '13"}