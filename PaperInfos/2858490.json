{"paperId":2858490,"citation":[],"reference":[{"content":"P.D. Adamczyk and B.P. Bailey. 2004. If not now, when?: The effects of interruption at different moments within task execution. In Conference on Human Factors in Computing Systems - Proceedings. 271--278.","paperID":"None"},{"content":"T. Batu, L. Fortnow, R. Rubinfeld, W.D. Smith, and P. White. 2013. Testing closeness of discrete distributions. Journal of the ACM (JACM) 60, 1 (2013), 4.","paperID":"None"},{"content":"Henry K Beller. 1971. Priming: Effects of advance information on matching. Journal of Experimental Psychology 87, 2 (1971), 176.","paperID":"None"},{"content":"A.J.a Berinsky, G.A.b Huber, and G.S.c Lenz. 2012. Evaluating online labor markets for experimental research: Amazon.com's mechanical turk. Political Analysis 20, 3 (2012), 351--368.","paperID":"None"},{"content":"P.J. Bickel and E. Levina. 2004. Some theory for Fisher's linear discriminant function, 'naive Bayes', and some alternatives when there are many more variables than observations. Bernoulli 10, 6 (12 2004), 989--1010.","paperID":"None"},{"content":"S Chan, I. Diakonikolas, P. Valiant, and G. Valiant. 2014. Optimal Algorithms for Testing Closeness of Discrete Distributions.. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms. 1193--1203.","paperID":"None"},{"content":"Dana Chandler and Adam Kapelner. 2013. Breaking monotony with meaning: Motivation in crowdsourcing markets. Journal of Economic Behavior & Organization 90 (2013), 123--133.","paperID":"None"},{"content":"J. Cheng, J. Teevan, and M. S. Bernstein. 2015. Measuring Crowdsourcing Effort with Error-Time Curves. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. ACM, 1365--1374.","paperID":"None"},{"content":"L.B. Chilton, R.C. Miller, J.J. Horton, and S. Azenkot. 2010. Task search in a human computation market. In Workshop Proceedings - Human Computation Workshop 2010, HCOMP2010. 1--9.","paperID":"None"},{"content":"Peng Dai, Jeffrey M Rzeszotarski, Praveen Paritosh, and Ed H Chi. 2015. And Now for Something Completely Different: Improving Crowdsourcing Workflows with Micro-Diversions. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing. ACM, 628--638.","paperID":"None"},{"content":"J. Davis, J. Arderiu, H. Lin, Z. Nevins, S. Schuon, O. Gallo, and M.-H. Yang. 2010. The HPU. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, CVPRW 2010. 9--16.","paperID":"None"},{"content":"S. Dow, A. Kulkarni, S. Klemmer, and B. Hartmann. 2012. Shepherding the crowd yields better work. In Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW. 1013--1022.","paperID":"None"},{"content":"C Felbaum. 1998. Wordnet, an Electronic Lexical Database. Cambridge: MIT Press.","paperID":"None"},{"content":"A. Finnerty, P. Kucherbaev, S. Tranquillini, and G. Convertino. 2013. Keep it simple: Reward and task design in crowdsourcing. In ACM International Conference Proceeding Series.","paperID":"None"},{"content":"Avniel S. Ghuman, Moshe Bar, Ian G. Dobbins, and David M. Schnyer. 2008. The effects of priming on frontal-temporal communication. Proceedings of the National Academy of Sciences 105, 24 (2008), 8405--8409.","paperID":"None"},{"content":"T. Hastie, R. Tibshirani, J. Friedman, T. Hastie, J. Friedman, and R. Tibshirani. 2009. The elements of statistical learning. Vol. 2. Springer.","paperID":"None"},{"content":"E. Huang, H. Zhang, D.C. Parkes, K.Z. Gajos, and Y. Chen. 2010. Toward automatic task design: A progress report. In Workshop Proceedings - Human Computation Workshop 2010, HCOMP2010. 77--85.","paperID":"None"},{"content":"D.E. Huber. 2008. Immediate Priming and Cognitive Aftereffects. Journal of Experimental Psychology: General 137, 2 (2008), 324--347.","paperID":"None"},{"content":"G. Kazai, J. Kamps, and N. Milic-Frayling. 2013. An analysis of human factors and label accuracy in crowdsourcing relevance judgments. Information Retrieval 16, 2 (2013), 138--178.","paperID":"None"},{"content":"P.J. Kellman and P. Garrigan. 2009. Perceptual learning and human expertise. Physics of life reviews 6, 2 (2009), 53--84.","paperID":"None"},{"content":"S. T. Kempley and John Morton. 1982. The effects of priming with regularly and irregularly related words in auditory word recognition. British Journal of Psychology 73, 4 (1982), 441--454.","paperID":"None"},{"content":"A. Kilgarriff. 1996. Comparing word frequencies across corpora: Why chi-square doesn't work, and an improved LOB-Brown comparison. In ALLC-ACH Conference.","paperID":"None"},{"content":"P. Kinnaird, L. Dabbish, and S. Kiesler. 2012. Workflow transparency in a microtask marketplace. In Proceedings of the ACM 2012 International Conference on Support Group Work. 281--284.","paperID":"None"},{"content":"A. Kittur, B. Smus, S. Khamkar, and R.E. Kraut. 2011. Crowdforge: Crowdsourcing complex work. In Proceedings of the 24th annual ACM symposium on User interface software and technology. ACM, 43--52.","paperID":"None"},{"content":"W.S. Lasecki, J.M. Rzeszotarski, A. Marcus, and J.P. Bigham. 2015. The Effects of Sequence and Delay on Crowd Work. CHI -- Human Factors in Computing Systems (2015).","paperID":"None"},{"content":"John Le, Andy Edmonds, Vaughn Hester, and Lukas Biewald. 2010. Ensuring quality in crowdsourced search relevance evaluation: The effects of training question distribution. In SIGIR 2010 workshop on crowdsourcing for search evaluation. 21--26.","paperID":"None"},{"content":"Sheena Lewis, Mira Dontcheva, and Elizabeth Gerber. 2011. Affective computational priming and creativity. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 735--744.","paperID":"None"},{"content":"W. Mason and D.J. Watts. 2009. Financial incentives and the \"performance of crowds\". In Proceedings of the ACM SIGKDD Workshop on Human Computation, HCOMP '09. 77--85.","paperID":"None"},{"content":"Susanne Mayr and Axel Buchner. 2007. Negative priming as a memory phenomenon: A review of 20 years of negative priming research. Zeitschrift fur Psychologie/Journal of Psychology 215, 1 (2007), 35.","paperID":"None"},{"content":"J.M. Mortensen, M.A. Musen, and N.F. Noy. 2013. Crowdsourcing the verification of relationships in biomedical ontologies. AMIA Annual Symposium proceedings (2013), 1020--1029.","paperID":"None"},{"content":"D. Powers. 1998. Applications and explanations of Zipf's law. In Proceedings of the joint conferences on new methods in language processing and computational natural language learning. Association for Computational Linguistics, 151--160.","paperID":"None"},{"content":"Rion Snow, Brendan O'Connor, Daniel Jurafsky, and Andrew Y Ng. 2008. Cheap and fast-but is it good?: evaluating non-expert annotations for natural language tasks. In Proceedings of the conference on empirical methods in natural language processing. Association for Computational Linguistics, 254--263.","paperID":"None"},{"content":"P.H. Thibodeau and L. Boroditsky. 2013. Natural language metaphors covertly influence reasoning. PloS one 8, 1 (2013), e52961.","paperID":"None"},{"content":"G. Valiant. 2012. Algorithmic Approaches to Statistical Questions. PhD Thesis. University of California at Berkely.","paperID":"None"},{"content":"S.C. Warby, S.L. Wendt, P. Welinder, E.G.S. Munk, O. Carrillo, H.B.D. Sorensen, P. Jennum, P.E. Peppard, P. Perona, and E. Mignot. 2014. Sleep-spindle detection: Crowdsourcing and evaluating performance of experts, non-experts and automated methods. Nature Methods 11, 4 (2014), 385--392.","paperID":"None"},{"content":"Clive Warren and John Morton. 1982. The effects of priming on picture recognition. British Journal of Psychology 73, 1 (1982), 117--129.","paperID":"None"},{"content":"S.M. Wolfson and M. Lease. 2011. Look before you leap: legal pitfalls of crowdsourcing. Proceedings of the American Society for Information Science and Technology 48, 1 (2011), 1--10.","paperID":"None"},{"content":"Ming Yin, Yiling Chen, and Yu-An Sun. 2014. Monetary interventions in crowdsourcing task switching. In Second AAAI Conference on Human Computation and Crowdsourcing.","paperID":"None"},{"content":"H. Zhang. 2004. The optimality of Naive Bayes. In Proceedings of the Seventeenth International Florida Artificial Intelligence Research Society Conference, FLAIRS 2004, Vol. 2. 562--567.","paperID":"None"},{"content":"G. K. Zipf. 1949. Human behavior and the principle of least effort. (1949).","paperID":"None"}],"abstract":"Microtask platforms are becoming commonplace tools for performing human research, producing gold-standard data, and annotating large datasets. These platforms connect requesters (researchers or companies) with large populations (crowds) of workers, who perform small tasks, typically taking less than five minutes each. A topic of ongoing research concerns the design of tasks that elicit high quality annotations. Here we identify a seemingly banal feature of nearly all crowdsourcing workflows that profoundly impacts workers' responses. Microtask assignments typically consist of a sequence of tasks sharing a common format (e.g., circle galaxies in an image). Using image-labeling, a canonical microtask format, we show that earlier tasks can have a strong influence on responses to later tasks, shifting the distribution of future responses by 30-50% (total variational distance). Specifically, prior tasks influence the content that workers focus on, as well as the richness and specialization of responses. We call this phenomenon intertask effects. We compare intertask effects to framing, effected by stating the requester's research interest, and find that intertask effects are on par or stronger. If uncontrolled, intertask effects could be a source of systematic bias, but our results suggest that, with appropriate task design, they might be leveraged to hone worker focus and acuity, helping to elicit reproducible, expert-level judgments. Intertask effects are a crucial aspect of human computation that should be considered in the design of any crowdsourced study.","title":"How One Microtask Affects Another","filename":"CHI16/p3155","authors":["Edward Newell","Derek Ruths"],"conference":"CHI '16"}