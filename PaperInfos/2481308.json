{"paperId":2481308,"citation":[{"content":"Zerrin Yumak , Jianfeng Ren , Nadia Magnenat Thalmann , Junsong Yuan, Tracking and fusion for multiparty interaction with a virtual character and a social robot, SIGGRAPH Asia 2014 Autonomous Virtual Humans and Social Robot for Telepresence, p.1-7, December 03-06, 2014, Shenzhen, China","paperID":"2668958"},{"content":"Iolanda Leite , Marissa McCoy , Daniel Ullman , Nicole Salomons , Brian Scassellati, Comparing Models of Disengagement in Individual and Group Interactions, Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction, March 02-05, 2015, Portland, Oregon, USA","paperID":"2696466"},{"content":"Nadia Magnenat Thalmann , Daniel Thalmann , Zerrin Yumak, Multimodal human-machine interaction including virtual humans or social robots, SIGGRAPH Asia 2014 Courses, p.1-103, December 03-06, 2014, Shenzhen, China","paperID":"2675052"},{"content":"K. Ruhland , C. E. Peters , S. Andrist , J. B. Badler , N. I. Badler , M. Gleicher , B. Mutlu , R. McDonnell, A Review of Eye Gaze in Virtual Agents, Social Robotics and HCI: Behaviour Generation, User Interaction and Perception, Computer Graphics Forum, v.34 n.6, p.299-326, September 2015","paperID":"2858875"}],"reference":[{"content":"Shivani Agarwal , Aatif Awan , Dan Roth, Learning to Detect Objects in Images via a Sparse, Part-Based Representation, IEEE Transactions on Pattern Analysis and Machine Intelligence, v.26 n.11, p.1475-1490, November 2004","paperID":"1025086"},{"content":"Sean Andrist , Tomislav Pejsa , Bilge Mutlu , Michael Gleicher, Designing effective gaze mechanisms for virtual agents, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, May 05-10, 2012, Austin, Texas, USA","paperID":"2207777"},{"content":"Nikolaus Bee , Johannes Wagner , Elisabeth André , Thurid Vogt , Fred Charles , David Pizzi , Marc Cavazza, Discovering eye gaze behavior during human-agent conversation in an interactive storytelling application, International Conference on Multimodal Interfaces and the Workshop on Machine Learning for Multimodal Interaction, November 08-10, 2010, Beijing, China","paperID":"1891915"},{"content":"Bennewitz, M., Faber, F., Joho, D., Schreiber, M. and Behnke, S. Towards a humanoid museum guide robot that interacts with multiple persons. In Proc. HUMANOIDS 2005, (2005), 418--423.","paperID":"None"},{"content":"Bohus, D. and Horvitz, E. Computational models for multiparty turn-taking. Microsoft Research Technical Report MSR-TR 2010--115, (2010).","paperID":"None"},{"content":"Dan Bohus , Eric Horvitz, Facilitating multiparty dialog with gaze, gesture, and speech, International Conference on Multimodal Interfaces and the Workshop on Machine Learning for Multimodal Interaction, November 08-10, 2010, Beijing, China","paperID":"1891910"},{"content":"Dan Bohus , Eric Horvitz, Learning to predict engagement with a spoken dialog system in open-world settings, Proceedings of the SIGDIAL 2009 Conference: The 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue, p.244-252, September 11-12, 2009, London, United Kingdom","paperID":"1708411"},{"content":"Dan Bohus , Eric Horvitz, Multiparty turn taking in situated dialog: study, lessons, and directions, Proceedings of the SIGDIAL 2011 Conference, p.98-109, June 17-18, 2011, Portland, Oregon","paperID":"2132903"},{"content":"Burgoon, J.K. and Dillman, L. Gender, immediacy, and nonverbal communication. in Kalbfleisch, P.J. and Cody, M.J. eds. Gender, power, and communication in human relationships, Psychology Press, 1995.","paperID":"None"},{"content":"Ginevra Castellano , André Pereira , Iolanda Leite , Ana Paiva , Peter W. McOwan, Detecting user engagement with a robot companion using task and social interaction-based features, Proceedings of the 2009 international conference on Multimodal interfaces, November 02-04, 2009, Cambridge, Massachusetts, USA","paperID":"1647336"},{"content":"Vijay Chidambaram , Yueh-Hsuan Chiang , Bilge Mutlu, Designing persuasive robots: how robots might persuade people using vocal and nonverbal cues, Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction, March 05-08, 2012, Boston, Massachusetts, USA","paperID":"2157798"},{"content":"Clark, H. and Carlson, T. Hearers and speech acts. Language, 58, (1982), 332--373.","paperID":"None"},{"content":"Chien-Ming Huang , Bilge Mutlu, Robot behavior toolkit: generating effective social behaviors for robots, Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction, March 05-08, 2012, Boston, Massachusetts, USA","paperID":"2157694"},{"content":"Kristiina Jokinen , Masafumi Nishida , Seiichi Yamamoto, On eye-gaze and turn-taking, Proceedings of the 2010 workshop on Eye gaze in intelligent human machine interaction, p.118-123, February 07-07, 2010, Hong Kong, China","paperID":"2002352"},{"content":"Michael Katzenmaier , Rainer Stiefelhagen , Tanja Schultz, Identifying the addressee in human-human-robot interactions based on head pose and speech, Proceedings of the 6th international conference on Multimodal interfaces, October 13-15, 2004, State College, PA, USA","paperID":"1027959"},{"content":"Kendon, A. Conducting Interaction: Patterns of Behavior in Focused Encounters. Cambridge University Press, 1990.","paperID":"None"},{"content":"Yoshinori Kuno , Kazuhisa Sadazuka , Michie Kawashima , Keiichi Yamazaki , Akiko Yamazaki , Hideaki Kuzuoka, Museum guide robot based on sociological interaction analysis, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, April 28-May 03, 2007, San Jose, California, USA","paperID":"1240804"},{"content":"Hideaki Kuzuoka , Yuya Suzuki , Jun Yamashita , Keiichi Yamazaki, Reconfiguring spatial formation arrangement by robot body orientation, Proceedings of the 5th ACM/IEEE international conference on Human-robot interaction, March 02-05, 2010, Osaka, Japan","paperID":"1734557"},{"content":"Sebastian Lang , Marcus Kleinehagenbrock , Sascha Hohenner , Jannik Fritsch , Gernot A. Fink , Gerhard Sagerer, Providing the basis for human-robot-interaction: a multi-modal attention system for a mobile robot, Proceedings of the 5th international conference on Multimodal interfaces, November 05-07, 2003, Vancouver, British Columbia, Canada","paperID":"958441"},{"content":"Langton, S.R., Watt, R.J. and Bruce, V. Do the eyes have it? Cues to the direction of social attention. Trends in Cognitive Sciences, 2, 2 (2003), 50--59.","paperID":"None"},{"content":"Min Kyung Lee , Sara Kiesler , Jodi Forlizzi, Receptionist or information kiosk: how do people talk with a robot?, Proceedings of the 2010 ACM conference on Computer supported cooperative work, February 06-10, 2010, Savannah, Georgia, USA","paperID":"1718927"},{"content":"Liyuan Li , Xinguo Yu , Jun Li , Gang Wang , Ji-Yu Shi , Yeow Kee Tan , Haizhou Li, Vision-based attention estimation and selection for social robot to perform natural interaction in the open world, Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction, March 05-08, 2012, Boston, Massachusetts, USA","paperID":"2157746"},{"content":"Michalowski, M.P., Sabanovic, S. and Simmons, R. A spatial model of engagement for a social robot. In Proc. 9th IEEE International Workshop on Advanced Motion Control, (2006), 762--767.","paperID":"None"},{"content":"Louis-Philippe Morency , C. Mario Christoudias , Trevor Darrell, Recognizing gaze aversion gestures in embodied conversational discourse, Proceedings of the 8th international conference on Multimodal interfaces, November 02-04, 2006, Banff, Alberta, Canada","paperID":"1181051"},{"content":"Jonathan Mumm , Bilge Mutlu, Human-robot proxemics: physical and psychological distancing in human-robot interaction, Proceedings of the 6th international conference on Human-robot interaction, March 06-09, 2011, Lausanne, Switzerland","paperID":"1957786"},{"content":"Bilge Mutlu , Takayuki Kanda , Jodi Forlizzi , Jessica Hodgins , Hiroshi Ishiguro, Conversational gaze mechanisms for humanlike robots, ACM Transactions on Interactive Intelligent Systems (TiiS), v.1 n.2, p.1-33, January 2012","paperID":"2070725"},{"content":"Bilge Mutlu , Toshiyuki Shiwa , Takayuki Kanda , Hiroshi Ishiguro , Norihiro Hagita, Footing in human-robot conversations: how robots might shape participant roles using gaze cues, Proceedings of the 4th ACM/IEEE international conference on Human robot interaction, March 09-13, 2009, La Jolla, California, USA","paperID":"1514109"},{"content":"Yukiko I. Nakano , Gabe Reinstein , Tom Stocky , Justine Cassell, Towards a model of face-to-face grounding, Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, p.553-561, July 07-12, 2003, Sapporo, Japan","paperID":"1075166"},{"content":"Yukiko I. Nakano , Ryo Ishii, Estimating user's engagement from eye-gaze behaviors in human-agent conversations, Proceedings of the 15th international conference on Intelligent user interfaces, February 07-10, 2010, Hong Kong, China","paperID":"1719990"},{"content":"Patterson, M. An arousal model of interpersonal intimacy. Psychological Review, 83, 3 (1976), 235--245.","paperID":"None"},{"content":"Christopher Peters, Direction of attention perception for conversation initiation in virtual environments, Lecture Notes in Computer Science, Springer-Verlag, London, UK, 2005","paperID":"1099295"},{"content":"Christopher Peters , Catherine Pelachaud , Elisabetta Bevacqua , Maurizio Mancini , Isabella Poggi, A model of attention and interest using Gaze behavior, Lecture Notes in Computer Science, Springer-Verlag, London, UK, 2005","paperID":"1099296"},{"content":"Charles Rich , Brett Ponsleur , Aaron Holroyd , Candace L. Sidner, Recognizing engagement in human-robot interaction, Proceedings of the 5th ACM/IEEE international conference on Human-robot interaction, March 02-05, 2010, Osaka, Japan","paperID":"1734580"},{"content":"Sidner, C. Engagement: Looking and not looking as evidence for disengagement. Workshop at HRI'12, (2012).","paperID":"None"},{"content":"Candace L. Sidner , Cory D. Kidd , Christopher Lee , Neal Lesh, Where to look: a study of human-robot engagement, Proceedings of the 9th international conference on Intelligent user interfaces, January 13-16, 2004, Funchal, Madeira, Portugal","paperID":"964458"},{"content":"Candace L. Sidner , Christopher Lee , Cory D. Kidd , Neal Lesh , Charles Rich, Explorations in engagement for humans and robots, Artificial Intelligence, v.166 n.1-2, p.140-164, August 2005","paperID":"1644610"},{"content":"Daniel Szafir , Bilge Mutlu, Pay attention!: designing adaptive agents that monitor and improve user engagement, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, May 05-10, 2012, Austin, Texas, USA","paperID":"2207679"},{"content":"Tojo, T., Matsusaka, Y., Ishii, T. and Kobayashi, T. A conversational robot utilizing facial and body expressions. IEEE Int. Conf. on Systems, Man, and Cybernetics (SMC), (2000), 858--863.","paperID":"None"},{"content":"Roel Vertegaal , Yaping Ding, Explaining effects of eye gaze on mediated group conversations:: amount or synchronization?, Proceedings of the 2002 ACM conference on Computer supported cooperative work, November 16-20, 2002, New Orleans, Louisiana, USA","paperID":"587085"},{"content":"Roel Vertegaal , Robert Slagter , Gerrit van der Veer , Anton Nijholt, Eye gaze patterns in conversations: there is more to conversational agents than meets the eyes, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, p.301-308, March 2001, Seattle, Washington, USA","paperID":"365119"},{"content":"Fumitaka Yamaoka , Takayuki Kanda , Hiroshi Ishiguro , Norihiro Hagita, How close?: model of proximity control for information-presenting robots, Proceedings of the 3rd ACM/IEEE international conference on Human robot interaction, March 12-15, 2008, Amsterdam, The Netherlands","paperID":"1349841"},{"content":"Fumitaka Yamaoka , Takayuki Kanda , Hiroshi Ishiguro , Norihiro Hagita, Developing a model of robot behavior to identify and appropriately respond to implicit attention-shifting, Proceedings of the 4th ACM/IEEE international conference on Human robot interaction, March 09-13, 2009, La Jolla, California, USA","paperID":"1514120"},{"content":"Fumitaka Yamaoka , Takayuki Kanda , Hiroshi Ishiguro , Norihiro Hagita, A model of proximity control for information-presenting robots, IEEE Transactions on Robotics, v.26 n.1, p.187-195, February 2010","paperID":"1771980"},{"content":"Akiko Yamazaki , Keiichi Yamazaki , Takaya Ohyama , Yoshinori Kobayashi , Yoshinori Kuno, A techno-sociological solution for designing a museum guide robot: regarding choosing an appropriate visitor, Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction, March 05-08, 2012, Boston, Massachusetts, USA","paperID":"2157800"},{"content":"Akiko Yamazaki , Keiichi Yamazaki , Yoshinori Kuno , Matthew Burdelski , Michie Kawashima , Hideaki Kuzuoka, Precision timing in human-robot interaction: coordination of head movement and utterance, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, April 05-10, 2008, Florence, Italy","paperID":"1357077"},{"content":"Keiichi Yamazaki , Akiko Yamazaki , Mai Okada , Yoshinori Kuno , Yoshinori Kobayashi , Yosuke Hoshi , Karola Pitsch , Paul Luff , Dirk vom Lehn , Christian Heath, Revealing Gauguin: engaging visitors in robot guide's explanation in an art museum, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, April 04-09, 2009, Boston, MA, USA","paperID":"1518919"},{"content":"Tomoko Yonezawa , Hirotake Yamazoe , Akira Utsumi , Shinji Abe, Evaluating crossmodal awareness of daily-partner robot to user's behaviors with gaze and utterance detection, Proceedings of the 3rd ACM International Workshop on Context-Awareness for Self-Managing Systems, p.1-8, May 11-11, 2009, Nara, Japan","paperID":"1538866"}],"abstract":"Recognizing users' engagement state and intentions is a pressing task for computational agents to facilitate fluid conversations in situated interactions. We investigate how to quantitatively evaluate high-level user engagement and intentions based on low-level visual cues, and how to design engagement-aware behaviors for the conversational agents to behave in a sociable manner. Drawing on machine learning techniques, we propose two computational models to quantify users' attention saliency and engagement intentions. Their performances are validated by a close match between the predicted values and the ground truth annotation data. Next, we design a novel engagement-aware behavior model for the agent to adjust its direction of attention and manage the conversational floor based on the estimated users' engagement. In a user study, we evaluated the agent's behaviors in a multiparty dialog scenario. The results show that the agent's engagement-aware behaviors significantly improved the effectiveness of communication and positively affected users' experience.","video":"http://www.youtube.com/embed/ugeusV1iYe4?rel=0","title":"Designing engagement-aware agents for multiparty conversations","filename":"CHI13/p2233","authors":["Qianli Xu","Liyuan Li","Gang Wang"],"conference":"CHI '13"}