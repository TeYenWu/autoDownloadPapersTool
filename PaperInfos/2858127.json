{"paperId":2858127,"citation":[],"abstract":"Video communication using head-mounted cameras could be useful to mediate shared activities and support collaboration. Growing popularity of wearable gaze trackers presents an opportunity to add gaze information on the egocentric video. We hypothesized three potential benefits of gaze-augmented egocentric video to support collaborative scenarios: support deictic referencing, enable grounding in communication, and enable better awareness of the collaborator's intentions. Previous research on using egocentric videos for real-world collaborative tasks has failed to show clear benefits of gaze point visualization. We designed a study, deconstructing a collaborative car navigation scenario, to specifically target the value of gaze-augmented video for intention prediction. Our results show that viewers of gaze-augmented video could predict the direction taken by a driver at a four-way intersection more accurately and more confidently than a viewer of the same video without the superimposed gaze point. Our study demonstrates that gaze augmentation can be useful and encourages further study in real-world collaborative scenarios.","reference":[{"content":"Susan E. Brennan, Xin Chen, Christopher A. Dickinson, Mark B. Neider, Gregory J. Zelinsky. 2008. Coordinating cognition: The costs and benefits of shared gaze during collaborative search. Cognition. Issue 3, 1465--1477. http://dx.doi.org/10.1016/j.cognition.2007.05.012","paperID":"None"},{"content":"JedR. Brubaker, Gina Venolia, and John C. Tang. 2012. Focusing on shared experiences: moving beyond the camerain video communication. In Proceedings of the Designing Interactive Systems Conference (DIS '12). 96--105. http://doi.acm.org/10.1145/2317956.2317973","paperID":"None"},{"content":"Andreas Bulling and Hans Gellersen. 2010. Toward mobile eye-based human-computer interaction. IEEE Pervasive Computing 9. http://doi.org/10.1109/MPRV.2010.86","paperID":"None"},{"content":"Roger M Cooper. 1974. The control of eye fixation by the meaning of spoken language: A new methodology for the real-time investigation of speech perception, memory, and language processing. Cognitive Psychology 6.1 (1974): 84--107. http://dx.doi.org/10.1016/0010-0285(74)90005-X","paperID":"None"},{"content":"Anup Doshi and Mohan Manubhai Trivedi. 2009. On the roles of eye gaze and head dynamics in predicting driver's intent to change lanes. IEEE Transactions on Intelligent Transportation System. 10, 3. 453--462. http://dx.doi.org/10.1109/TITS.2009.2026675","paperID":"None"},{"content":"Pat Dugard. 2014. Randomization tests: Anew gold standard? Journal of Contextual Behavioral Science 3, 1: 65--68. http://doi.org/10.1016/j.jcbs.2013.10.001","paperID":"None"},{"content":"Tom Foulsham, Esther Walker, Alan Kingstone. 2011. The where, what and when of gaze allocation in the lab and the natural environment. Vision Research. 51, 17. 1920--1931. http://dx.doi.org/10.1016/j.visres.2011.07.002.","paperID":"None"},{"content":"FOVE virtual reality headset. http://www.getfove.com/ (accessed: 5 January 2016)","paperID":"None"},{"content":"Susan R. Fussell and Leslie D. Setlock. 2003. Using Eye-Tracking Techniques to Study Collaboration on Physical Tasks: Implications for Medical Research. Unpublished manuscript, Carnegie Mellon University, (2003), 1--25.","paperID":"None"},{"content":"Susan R. Fussell, Robert E. Kraut, and Jane Siegel. 2000. Coordination of communication: effects of shared visual context on collaborative work. In Proceedings of Computer supported cooperative work (CSCW '00), 21--30. http://doi.acm.org/10.1145/358916.358947.","paperID":"None"},{"content":"Susan R. Fussell, Leslie D. Setlock, and Robert E. Kraut. 2003. Effects of head-mounted and scene oriented video systems on remote collaboration on physical tasks. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '03). 513--520. http://doi.acm.org/10.1145/642611.642701.","paperID":"None"},{"content":"DarrenGergle, Robert E. Kraut, Susan R. Fussell. 2013. Using Visual Information for Grounding and Awareness in Collaborative Tasks. Human-Computer Interaction. 28, 1. 1--39","paperID":"None"},{"content":"Zenzi M. Griffinand Kathryn Bock. 2000. What the eyes say about speaking. Psychological science. 11, 4. 274--279. http://dx.doi.org/10.1111/1467--9280.00255.","paperID":"None"},{"content":"Mary M. Hayhoe, Anurag Shrivastava, Ryan Mruczek, Jeff B. Pelz. 2003. Visual memory and motor planning in a natural task. Journal of Vision. 3. 49--63. http://dx.doi.org/10.1167/3.1.6.","paperID":"None"},{"content":"Kori Inkpen, Brett Taylor, Sasa Junuzovic, John Tang, and Gina Venolia. 2013. Experiences2Go: sharing kids' activities outside the home with remote family members. In Proceedings of the 2013conference on Computer supported cooperative work (CSCW '13). 1329--1340. http://doi.acm.org/10.1145/2441776.2441926.","paperID":"None"},{"content":"Steven Johnson, Madeleine Gibson, and Bilge Mutlu. 2015. Handheld or Hands free?: Remote Collaboration via Lightweight Head-Mounted Displays and Handheld Devices. In Proceedings of Computer Supported Cooperative Work & Social Computing (CSCW '15). 1825--1836. http://doi.acm.org/10.1145/2675133.2675176.","paperID":"None"},{"content":"Brennan Jones, Anna Witcraft, Scott Bateman, Carman Neustaedter, and Anthony Tang. 2015. Mechanics of Camera Work in Mobile Video Collaboration. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI'15). 957--966. http://doi.acm.org/10.1145/2702123.2702345.","paperID":"None"},{"content":"Tsuneo Kito, Masahiro Haraguchi, Takayuki Funatsu, Motoharu Sato, Michiaki Kondo. 1989. Measurements of gaze movements while driving. Perceptual and Motor Skills. 68.1. 19--25. http://dx.doi.org/10.2466/pms.1989.68.1.19.","paperID":"None"},{"content":"Michael F. Land. 2006. Eye movements and the control of actions in everyday life. Progress in Retinal and Eye Research. 25, 3, 296--324. http://dx.doi.org/10.1016/j.preteyeres.2006.01.002.","paperID":"None"},{"content":"Teesid Leelasawassuk, Dima Damen, Walterio W. Mayol-Cuevas. 2015. Estimating visual attention from a head mounted IMU. In Proceedings of International Symposium on Wearable Computers (ISWC '15). 147150. http://doi.acm.org/10.1145/2802083.2808394.","paperID":"None"},{"content":"Yin Li, Alireza Fathi, and James M. Rehg. 2013. Learning to Predict Gaze in Egocentric Video. In Proceedings of IEEE International Conference on Computer Vision (ICCV'13). 3216--3223. http://dx.doi.org/10.1109/ICCV.2013.399.","paperID":"None"},{"content":"ChristianLicoppe and JulienMorel. 2009. The collaborative work of producing meaningful shots in mobile video telephony. In Proceedings of International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI'09). Article 35. http://doi.acm.org/10.1145/1613858.1613903.","paperID":"None"},{"content":"Paul Luff, Christian Heath, David Greatbatch. 1992. Tasks-in-interaction: paper and screen based documentation in collaborative activity. In Proceedings of ACM conference on Computer-supported cooperative work (CSCW '92). 163--170. http://doi.acm.org/10.1145/143457.143475.","paperID":"None"},{"content":"Neil Mennie, Mary Hayhoe, Brian Sullivan. 2007. Look-ahead fixations: anticipatory eye movements in natural tasks. Experimental Brain Research. 179(3). 427--442. http://dx.doi.org/10.1007/s00221-006-0804-0.","paperID":"None"},{"content":"Bonnie A. Nardi, Heinrich Schwarz, Allan Kuchinsky, Robert Leichner, Steve Whittaker, Robert Sclabassi. 1993. Turning away from talking heads: the use of video-as-data in neurosurgery. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI'93). 327--334. http://doi.acm.org/10.1145/169059.169261.","paperID":"None"},{"content":"Mark B. Neider, Xin Chen, Christopher A. Dickinson, Susan E. Brennan, Gregory J. Zelinsky. 2010, Coordinating spatial referencing using shared gaze. Psychonomic Bulletin & Review. 17(5). 718--724. http://dx.doi.org/10.3758/PBR.17.5.718.","paperID":"None"},{"content":"Carman Neustaedter and Tejinder K. Judge. 2010. Peek-A-Boo: the design of a mobile family media space. In Proceedings of the 12th ACM international conference adjunct papers on Ubiquitous computing Adjunct (UbiComp'10 Adjunct). 449--450. http://doi.acm.org/10.1145/1864431.1864482.","paperID":"None"},{"content":"Kenton O'Hara, Alison Black, and Matthew Lipson. 2006. Everyday practices with mobile video telephony. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI'06). 871--880. http://doi.acm.org/10.1145/1124772.1124900.","paperID":"None"},{"content":"Jeff B. Pelz, Roxanne Canosa. 2001. Oculomotor Behavior and Perceptual Strategies in Complex Tasks. Vision Research. 41 (25). 3587--96. http://dx.doi.org/10.1016/S0042--6989(01)00245-0","paperID":"None"},{"content":"Bastian Pfleging, Stefan Schneegass, and Albrecht Schmidt. 2013. Exploring user expectations for context and road video sharing while calling and driving. In Proceedings of the International Conference on Automotive User Interfaces and Interactive Vehicular Applications (AutomotiveUI'13). 132--139. http://doi.acm.org/10.1145/2516540.2516547.","paperID":"None"},{"content":"Jason Procyk, Carman Neustaedter, CarolynPang, Anthony Tang, Tejinder K. Judge. 2014. Exploring video streaming in public settings: shared geocaching over distance using mobile video chat. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI'14). 2163--2172. http://doi.acm.org/10.1145/2556288.2557198.","paperID":"None"},{"content":"Pernilla Qvarfordt, David Beymer, and Shumin Zhai. 2005. Real Tourist: a study of augmenting human human and human-computer dialogue with eye-gaze overlay. In Proceedings of the International conference on Human-Computer Interaction (INTERACT'05). 767--780. http://dx.doi.org/10.1007/11555261_61.","paperID":"None"},{"content":"Natalie Sebanza,Harold Bekkeringb, Günther Knoblicha. 2006. Joint action: bodies and minds moving together. Trends in Cognitive Sciences. 10(2). 70--76. http://dx.doi.org/10.1016/j.tics.2005.12.009.","paperID":"None"},{"content":"Kshitij Sharma, Jermann Patrick and Dillenbourg Pierre. 2015. Displaying Teacher's Gaze in a MOOC: Effects on Students' Video Navigation Patterns. In Proceedings of the 10th European Conference on Technology Enhanced Learning.","paperID":"None"},{"content":"Randy Steinand Susan E. Brennan. 2004. Another person's eye gaze as a cue in solving programming problems. In Proceedings of the6th international conference on Multimodal interfaces (ICMI'04). 9--15. http://doi.acm.org/10.1145/1027933.1027936.","paperID":"None"},{"content":"Rainer Stiefelhagen and Jie Zhu. 2002. Head orientation and gaze direction in meetings. In CHI '02 Extended Abstracts on Human Factors in Computing Systems (CHIEA '02). 858--859. http://doi.acm.org/10.1145/506443.506634.","paperID":"None"},{"content":"Karl Verfaillie and Anja Daems. 2002. Representing and anticipating human actions in vision. Visual Cognition. 9(1). 217--232. http://dx.doi.org/10.1080/13506280143000403.","paperID":"None"},{"content":"Kentaro Yamada, Yusuke Sugano, Takahiro Okabe, Yoichi Sato, Akihiro Sugimoto, Kazuo Hiraki. 2011. Attention Prediction in Egocentric Video Using Motion and Visual Saliency. Advances in Image and Video technology. http://dx.doi.org/10.1007/978--3--64225367--6_25.","paperID":"None"},{"content":"Xianjun Sam Zheng, PatrikMatos da Silva, Cedric Foucault, Siddharth Dasari, Meng Yuan, Stuart Goose. 2015. Wearable Solution for Industrial Maintenance. In Proceedings of Extended Abstracts on Human Factors in Computing Systems (CHIEA '15). 311--314. http://doi.acm.org/10.1145/2702613.2725442.","paperID":"None"}],"title":"Gaze Augmentation in Egocentric Video Improves Awareness of Intention","filename":"CHI16/p1573","authors":["Deepak Akkil","Poika Isokoski"],"conference":"CHI '16"}