{"paperId":2858584,"citation":[],"reference":[{"content":"W. Abd-Almageed, M.S. Fadali, and G. Bebis. 2002. A non-intrusive Kalman filter-based tracker for pursuit eye movement. In American Control Conference, 2002. Proceedings of the 2002, Vol. 2. 1443--1447 vol.2. DOI: http://dx.doi.org/10.1109/ACC.2002.1023224","paperID":"None"},{"content":"Daniel L. Ashbrook. 2010. Enabling Mobile Microinteractions. Ph.D. Dissertation. Atlanta, GA, USA. Advisor(s) Starner, Thad E. AAI3414437.","paperID":"None"},{"content":"Richard Bates and Howell O Istance. 2003. Why are eye mice unpopular? A detailed comparison of head and eye controlled assistive technology pointing devices. UAIS 2, 3 (2003), 280--290.","paperID":"None"},{"content":"L Burmark. 2004. Visual Presentations That Prompt, Flash & Transform Here are some great ways to have more visually interesting class sessions. Media and methods 40 (2004), 4--5.","paperID":"None"},{"content":"T. Cecchin, D. Sauter, D. Brie, and B. Martin. 1990. On-line Separation Of Smooth Pursuit And Saccadic Eye Movements. In Engineering in Medicine and Biology Society, 1990., Proceedings of the Twelfth Annual International Conference of the IEEE. 777--778. DOI: http://dx.doi.org/10.1109/IEMBS.1990.691327","paperID":"None"},{"content":"M. Cheng and J.S. Outerbridge. 1975. Optokinetic nystagmus during selective retinal stimulation. Experimental Brain Research 23, 2 (1975), 129--139. DOI:http://dx.doi.org/10.1007/BF00235455","paperID":"None"},{"content":"Charles E Connor, Howard E Egeth, and Steven Yantis. 2004. Visual attention: bottom-up versus top-down. Current Biology 14, 19 (2004), R850-R852. DOI: http://dx.doi.org/doi:10.1016/j.cub.2004.09.041","paperID":"None"},{"content":"Heiko Drewes and Albrecht Schmidt. 2007. Interacting with the Computer Using Gaze Gestures. In Human-Computer Interaction INTERACT 2007. Lecture Notes in Computer Science, Vol. 4663. Springer Berlin Heidelberg, 475-488. DOI: http://dx.doi.org/10.1007/978-3-540-74800-7_43","paperID":"None"},{"content":"Johannes Harms, Martina Kratky, Christoph Wimmer, Karin Kappel, and Thomas Grechenig. 2015. Navigation in Long Forms on Smartphones: Scrolling Worse than Tabs, Menus, and Collapsible Fieldsets. In INTERACT 2015. Lecture Notes in Computer Science, Vol. 9298. Springer, 333--340. DOI: http://dx.doi.org/10.1007/978-3-319-22698-9_21","paperID":"None"},{"content":"Sandra G Hart and Lowell E Staveland. 1988. Development of NASA-TLX (Task Load Index): Results of empirical and theoretical research. Advances in psychology 52 (1988), 139--183. DOI:http: //dx.doi.org/doi:10.1016/S0166-4115(08)62386-9","paperID":"None"},{"content":"Aulikki Hyrskykari, Päivi Majaranta, and Kari-Jouko Räihä. 2005. From gaze control to attentive interfaces. In Proceedings of HCII, Vol. 2.","paperID":"None"},{"content":"Robert J. K. Jacob. 1990. What You Look at is What You Get: Eye Movement-based Interaction Techniques. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '90). ACM, New York, NY, USA, 11--18. DOI: http://dx.doi.org/10.1145/97243.97246","paperID":"None"},{"content":"Shahram Jalaliniya, Diako Mardanbegi, and Thomas Pederson. 2015. MAGIC Pointing for Eyewear Computers. In Proceedings of the 2015 ACM International Symposium on Wearable Computers (ISWC '15). ACM, New York, NY, USA, 155--158. DOI:http://dx.doi.org/10.1145/2802083.2802094","paperID":"None"},{"content":"Shahram Jalaliniya, Diako Mardanbegi, Thomas Pederson, and Dan Witzner. 2014. Head and Eye Movement as Pointing Modalities for Eyewear Computers. In Wearable and Implantable Body Sensor Networks Workshops (BSN Workshops), 2014 11th International Conference on. 50--53. DOI: http://dx.doi.org/10.1109/BSN.Workshops.2014.14","paperID":"None"},{"content":"Do Hyong Koh, Sandeep Munikrishne Gowda, and Oleg V. Komogortsev. 2010. Real Time Eye Movement Identification Protocol. In CHI '10 Extended Abstracts on Human Factors in Computing Systems (CHI EA '10). ACM, New York, NY, USA, 3499--3504. DOI: http://dx.doi.org/10.1145/1753846.1754008","paperID":"None"},{"content":"Lori A Lott and Robert B Post. 1993. Up-down asymmetry in vertical induced motion. PERCEPTION-LONDON- 22 (1993), 527--527.","paperID":"None"},{"content":"PaulP. Maglio, Teenie Matlock, ChristopherS. Campbell, Shumin Zhai, and BartonA. Smith. 2000. Gaze and Speech in Attentive User Interfaces. In Advances in Multimodal Interfaces ICMI 2000. Lecture Notes in Computer Science, Vol. 1948. Springer Berlin Heidelberg, 1--7. DOI: http://dx.doi.org/10.1007/3-540-40063-X_1","paperID":"None"},{"content":"Diako Mardanbegi, Dan Witzner Hansen, and Thomas Pederson. 2012. Eye-based Head Gestures. In Proceedings of the Symposium on Eye Tracking Research and Applications (ETRA '12). ACM, New York, NY, USA, 139--146. DOI: http://dx.doi.org/10.1145/2168556.2168578","paperID":"None"},{"content":"Thomas Paul, Daniel Puscher, and Thorsten Strufe. 2015. The User Behavior in Facebook and its Development from 2009 until 2014. arXiv preprint arXiv:1505.04943 (2015).","paperID":"None"},{"content":"Ravikrishna Ruddarraju, Antonio Haro, Kris Nagel, Quan T. Tran, Irfan A. Essa, Gregory Abowd, and Elizabeth D. Mynatt. 2003. Perceptual User Interfaces Using Vision-based Eye Tracking. In Proceedings of ICMI '03. ACM, New York, NY, USA, 227-233. DOI: http://dx.doi.org/10.1145/958432.958475","paperID":"None"},{"content":"Javier San Agustin. 2009. Off-the-shelf gaze interaction. Ph.D. Dissertation. IT-Universitetet i KøbenhavnIT University of Copenhagen, DirektionenManagement, InstituttetThe Department, Innovative CommunicationInnovative Communication.","paperID":"None"},{"content":"Christopher A Sanchez and James Z Goolsbee. 2010. Character size and reading to remember from small displays. Computers & Education 55, 3 (2010), 1056--1062. DOI:http://dx.doi.org/doi: 10.1016/j.compedu.2010.05.001","paperID":"None"},{"content":"Albrecht Schmidt. 2005. Interactive context-aware systems interacting with ambient intelligence. Ambient intelligence 159 (2005).","paperID":"None"},{"content":"Jeffrey S Shell, Roel Vertegaal, and Alexander W Skaburskis. 2003. EyePliances: attention-seeking devices that respond to visual attention. In CHI'03 extended abstracts on Human factors in computing systems. ACM, 770--771.","paperID":"None"},{"content":"Linda E. Sibert and Robert J. K. Jacob. 2000. Evaluation of Eye Gaze Interaction. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '00). ACM, New York, NY, USA, 281--288. DOI: http://dx.doi.org/10.1145/332040.332445","paperID":"None"},{"content":"JWG Ter Braak. 1936. Untersuchungen über optokinetischen Nystagmus. Arch Neerl Physiol 21 (1936), 309--376.","paperID":"None"},{"content":"Mloédie Vidal, Andreas Bulling, and Hans Gellersen. 2012. Detection of Smooth Pursuits Using Eye Movement Shape Features. In Proceedings of the Symposium on Eye Tracking Research and Applications (ETRA '12). ACM, New York, NY, USA, 177--180. DOI:http://dx.doi.org/10.1145/2168556.2168586","paperID":"None"},{"content":"Mélodie Vidal, Andreas Bulling, and Hans Gellersen. 2013. Pursuits: Spontaneous Interaction with Displays Based on Smooth Pursuit Eye Movement and Moving Targets. In Proceedings of UbiComp '13. ACM, New York, NY, USA, 439--448. DOI: http://dx.doi.org/10.1145/2493432.2493477","paperID":"None"},{"content":"Shumin Zhai, Carlos Morimoto, and Steven Ihde. 1999. Manual and Gaze Input Cascaded (MAGIC) Pointing. In Proc. of the CHI '99. ACM, 246--253. DOI: http://dx.doi.org/10.1145/302979.303053","paperID":"None"},{"content":"Yanxia Zhang, Andreas Bulling, and Hans Gellersen. 2014. Pupil-canthi-ratio: A Calibration-free Method for Tracking Horizontal Gaze Direction. In Proceedings of the 2014 International Working Conference on Advanced Visual Interfaces (AVI '14). ACM, New York, NY, USA, 129--132. DOI: http://dx.doi.org/10.1145/2598153.2598186","paperID":"None"}],"abstract":"EyeGrip proposes a novel and yet simple technique of analysing eye movements for automatically detecting the user's objects of interest in a sequence of visual stimuli moving horizontally or vertically in front of the user's view. We assess the viability of this technique in a scenario where the user looks at a sequence of images moving horizontally on the display while the user's eye movements are tracked by an eye tracker. We conducted an experiment that shows the performance of the proposed approach. We also investigated the influence of the speed and maximum number of visible images in the screen, on the accuracy of EyeGrip. Based on the experiment results, we propose guidelines for designing EyeGrip-based interfaces. EyeGrip can be considered as an implicit gaze interaction technique with potential use in broad range of applications such as large screens, mobile devices and eyewear computers. In this paper, we demonstrate the rich capabilities of EyeGrip with two example applications: 1) a mind reading game, and 2) a picture selection system. Our study shows that by selecting an appropriate speed and maximum number of visible images in the screen the proposed method can be used in a fast scrolling task where the system accurately (87%) detects the moving images that are visually appealing to the user, stops the scrolling and brings the item(s) of interest back to the screen.","title":"EyeGrip: Detecting Targets in a Series of Uni-directional Moving Objects Using Optokinetic Nystagmus Eye Movements","filename":"CHI16/p5801","authors":["Shahram Jalaliniya","Diako Mardanbegi"],"conference":"CHI '16"}